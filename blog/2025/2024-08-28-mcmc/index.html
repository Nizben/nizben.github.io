<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="1-introduction">1. Introduction</h2> <p>This post (or rather a collection of notes) is an attempt to go through different concepts around <strong>MCMC methods</strong> from the ground up. I will be trying to gather and structure my learnings around this topic in a way that is clear, intelligible, and beginner friendly.</p> <p>Sampling from complex, high-dimensional probability distributions is a fundamental challenge in statistics, machine learning, and many applied fields. The core idea is that Markov Chain Monte Carlo (MCMC) methods overcome this challenge by constructing a Markov chain whose stationary distribution is the target distribution.</p> <p>In practice, MCMC is used for Bayesian inference, uncertainty quantification, and solving inverse problems in areas such as audio and image processing. This blogpost proposes an accessible introduction to MCMC methods from its theoretical foundations to some Python implementations of these algorithms for real world use cases.</p> <h2 id="2-theoretical-foundations">2. Theoretical Foundations</h2> <h3 id="21-markov-chains-and-stationarity">2.1. Markov Chains and Stationarity</h3> <p>A <strong>Markov chain</strong> is a sequence of random variables ${X_t}_{t \geq 0}$ that satisfies the Markov property:</p> \[P(X_{t+1} = y \mid X_t = x, X_{t-1} = x_{t-1}, \dots) = P(X_{t+1}=y \mid X_t=x).\] <p>The chain’s dynamics are defined by a <strong>transition probability</strong> $P(x,y)$ (or kernel) that is time-independent.</p> <p>A probability distribution $\pi(x)$ is said to be <strong>stationary</strong> (or invariant) if, for all states $y$,</p> \[\pi(y) = \sum_{x} \pi(x) P(x, y)\] <p>This means that if : $X_0 \sim \pi, \text{ then } X_t \sim \pi \text{ for every } t.$</p> <h3 id="22-detailed-balance-and-invariance">2.2. Detailed Balance and Invariance</h3> <p>A sufficient condition for stationarity is the <strong>detailed balance condition</strong>:</p> \[\pi(x) P(x, y) = \pi(y) P(y, x), \quad \forall x, y\] <p><strong>Proof Sketch:</strong></p> <ol> <li> <strong>Assume detailed balance:</strong> For every pair $(x,y)$:</li> </ol> \[\pi(x) P(x, y) = \pi(y) P(y, x)\] <ol> <li><strong>Sum over all $x$ for a fixed $y$:</strong></li> </ol> \[\sum_{x} \pi(x) P(x, y) = \sum_{x} \pi(y) P(y, x) = \pi(y) \sum_{x} P(y, x)\] <ol> <li> <strong>Normalization:</strong> Since $\sum_{x} P(y, x) = 1$, we obtain:</li> </ol> \[\sum_{x} \pi(x) P(x, y) = \pi(y)\] <p>Thus, detailed balance guarantees that $\pi$ is invariant under the chain dynamics.</p> <h3 id="23-convergence-and-ergodicity">2.3. Convergence and Ergodicity</h3> <p>For the empirical averages from the chain to converge to expectations under $\pi$, the chain must be:</p> <ul> <li> <strong>Irreducible:</strong> Every state can be reached from any other state.</li> <li> <strong>Aperiodic:</strong> The chain does not get trapped in cycles.</li> <li> <strong>Positive Recurrent:</strong> The expected return time to any state is finite.</li> </ul> <p>If these conditions hold, the <strong>ergodic theorem</strong> asserts that for any integrable function $f$:</p> \[\frac{1}{N}\sum_{t=1}^{N} f(X_t) \longrightarrow \mathbb{E}_{\pi}[f(x)]\] <p>as $N \to \infty$.</p> <p>This is the foundation behind using MCMC to approximate integrals and expectations.</p> <h2 id="3-mcmc-algorithms">3. MCMC Algorithms</h2> <p>Below are several common MCMC algorithms with some theroretical details, context, and pseudocode.</p> <h3 id="31-importance-sampling">3.1. Importance Sampling</h3> <p><strong>Context &amp; Objective:</strong></p> <p>Often, our goal is to compute expectations under a complex target distribution $\pi(x)$. For example, we may wish to evaluate:</p> \[I = \int f(x)\pi(x)\,dx\] <p>When direct sampling from $\pi(x)$ is infeasible, we introduce a proposal (or importance) distribution $q(x)$ that is easier to sample from.</p> <p><strong>Theory &amp; Formulation:</strong></p> <p>Given samples $x_1, x_2, \dots, x_N$ drawn from $q(x)$, the expectation is estimated as:</p> \[I\approx \frac{\sum_{i=1}^{N} f(x_i) w(x_i)}{\sum_{i=1}^{N} w(x_i)},\] <p>with importance weights defined by:</p> \[w(x) = \frac{\pi(x)}{q(x)}\] <p><em>Key Considerations:</em></p> <ul> <li> <strong>Choice of $q(x):q(x)$</strong> must have heavier tails than $\pi(x)$ to avoid extremely high weights.</li> <li> <strong>Variance:</strong> A poor choice of $q(x)$ leads to high variance in estimates.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">For</span> <span class="nv">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="nv">to</span> <span class="nv">N:</span>
    <span class="nv">Sample</span> <span class="nv">x_i</span> <span class="o">~</span> <span class="sx">q(x)</span>
    <span class="nv">Compute</span> <span class="nv">weight</span> <span class="nv">w_i</span> <span class="o">=</span> <span class="err">π</span><span class="p">(</span><span class="nv">x_i</span><span class="p">)</span> <span class="o">/</span> <span class="sx">q(x_i)</span>
<span class="nv">Estimate</span> <span class="nv">I</span> <span class="err">≈</span> <span class="p">(</span><span class="err">Σ</span> <span class="nv">f</span><span class="p">(</span><span class="nv">x_i</span><span class="p">)</span> <span class="nv">w_i</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="err">Σ</span> <span class="nv">w_i</span><span class="p">)</span>
</code></pre></div></div> <h3 id="32-metropolishastings-mh">3.2. Metropolis–Hastings (MH)</h3> <p><strong>Context &amp; Objective:</strong></p> <table> <tbody> <tr> <td>MH constructs a Markov chain whose stationary distribution is $\pi(x)$. It proposes moves using a proposal density $q(x’</td> <td>x)$ and accepts these moves with a carefully designed acceptance probability.</td> </tr> </tbody> </table> <p>The key idea is to design the transition probability $P(x \to x’)$ so that the target distribution is invariant. A sufficient condition is <strong>detailed balance</strong>, which states that for all states $x$ and $x’$:</p> <p>$\pi(x) P(x \to x’) = \pi(x’) P(x’ \to x)$</p> <p>In MH, the transition probability is given by:</p> <table> <tbody> <tr> <td>$P(x \to x’) = q(x’</td> <td>x) \alpha(x, x’)$</td> </tr> </tbody> </table> <p>where $\alpha(x, x’)$ is the acceptance probability. To satisfy detailed balance, we require:</p> <table> <tbody> <tr> <td>$\pi(x) q(x’</td> <td>x) \alpha(x, x’) = \pi(x’) q(x</td> <td>x’) \alpha(x’, x)$</td> </tr> </tbody> </table> <p>A common and effective choice is to define $\alpha(x, x’)$ as:</p> <table> <tbody> <tr> <td>$\alpha(x, x’) = \min\left{1, \frac{\pi(x’) \, q(x</td> <td>x’)}{\pi(x) \, q(x’</td> <td>x)}\right}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>When $\pi(x’) q(x</td> <td>x’) \geq \pi(x) q(x’</td> <td>x)$, we have $\alpha(x, x’) = 1$; otherwise, the move is accepted</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>with probability $\frac{\pi(x’) \, q(x</td> <td>x’)}{\pi(x) \, q(x’</td> <td>x)}$.</td> </tr> </tbody> </table> <p><strong>Convergence and ergodicity</strong></p> <p>The MH algorithm constructs a Markov chain that, under suitable conditions (irreducibility, aperiodicity, and positive recurrence), converges to the target distribution $\pi(x)$. The ergodic theorem then guarantees that time averages computed from the chain will converge to the expectations under $\pi(x)$:</p> \[\frac{1}{N}\sum_{t=1}^{N} f(x_t) \longrightarrow \mathbb{E}_{\pi}[f(x)] \quad \text{as } N \to \infty\] <p><strong>Practical considerations</strong></p> <ul> <li> <table> <tbody> <tr> <td>**Choice of Proposal $q(x’</td> <td>x)$ :** The efficiency of the MH algorithm heavily depends on how well the proposal distribution explores the state space. If $q(x’</td> <td>x)$ is too narrow, the chain will explore slowly, if too wide, the acceptance rate may drop.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Symmetric Proposals:</strong> When $q(x’</td> <td>x) = q(x</td> <td>x’)$ (as in a Gaussian random walk), the acceptance probability simplifies to:</td> </tr> </tbody> </table> </li> </ul> \[\alpha(x, x') = \min\left\{1, \frac{\pi(x')}{\pi(x)}\right\}\] <p><strong>Algorithm &amp; Pseudocode:</strong></p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Initialize</span><span class="w"> </span><span class="nv">x</span><span class="err">₀</span><span class="w">
</span><span class="nb">For</span><span class="w"> </span><span class="nv">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nb">N</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="w">
    </span><span class="nv">Propose</span><span class="w"> </span><span class="nv">x</span><span class="o">'</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="nv">q</span><span class="p">(</span><span class="nv">x</span><span class="o">'|</span><span class="nv">x</span><span class="o">_</span><span class="nv">t</span><span class="p">)</span><span class="w">
    </span><span class="nv">Calculate</span><span class="w"> </span><span class="nv">acceptance</span><span class="w"> </span><span class="nv">probability</span><span class="o">:</span><span class="w">
        </span><span class="err">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">min</span><span class="p">{</span><span class="m">1</span><span class="o">,</span><span class="w"> </span><span class="p">[</span><span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="o">'</span><span class="p">)</span><span class="w"> </span><span class="nv">q</span><span class="p">(</span><span class="nv">x</span><span class="o">_</span><span class="nv">t</span><span class="o">|</span><span class="nv">x</span><span class="o">'</span><span class="p">)]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">[</span><span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="o">_</span><span class="nv">t</span><span class="p">)</span><span class="w"> </span><span class="nv">q</span><span class="p">(</span><span class="nv">x</span><span class="o">'|</span><span class="nv">x</span><span class="o">_</span><span class="nv">t</span><span class="p">)]</span><span class="w"> </span><span class="p">}</span><span class="w">
    </span><span class="bp">With</span><span class="w"> </span><span class="nv">probability</span><span class="w"> </span><span class="err">α</span><span class="o">:</span><span class="w">
        </span><span class="nb">Set</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">x</span><span class="o">'</span><span class="w">
    </span><span class="nv">Else</span><span class="o">:</span><span class="w">
        </span><span class="nb">Set</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="nv">t</span><span class="w">
</span></code></pre></div></div> <h3 id="33-gibbs-sampling">3.3. Gibbs Sampling</h3> <p><strong>Context &amp; Objective:</strong></p> <p>Gibbs sampling is a special case of the Metropolis–Hastings algorithm, optimized for high-dimensional problems where the joint distribution $\pi(x)$ is difficult to sample from directly, but the full conditional distributions $π(x_i∣x_{−i})$ (where $x_{-i}$ denotes all components except $x_i$) are tractable.</p> <p>Suppose $x = (x_1, x_2, \dots, x_d)$ is a $d$-dimensional vector. The Gibbs sampler iterates over each coordinate and updates it by sampling from the full conditional distribution:</p> \[x_i^{(t+1)} \sim \pi\left(x_i \mid x_1^{(t+1)}, \dots, x_{i-1}^{(t+1)}, x_{i+1}^{(t)}, \dots, x_d^{(t)}\right)\] <p>Since each update is drawn exactly from the full conditional, the move is automatically accepted. The chain is constructed to have $\pi(x)$ as its stationary distribution.</p> <p><strong>Theoretical details:</strong></p> <ul> <li> <strong>Consistency of conditionals:</strong> For Gibbs sampling to work, the set of full conditionals must be consistent with a joint distribution $\pi(x)$. Under this condition, if the chain is run long enough the joint ditribution of the samples converges to $\pi(x)$.</li> <li> <strong>Detailed balance in Gibbs sampling:</strong> Although Gibbs sampling does not require an explicit acceptance step, one can show that it satisfies detailed balance. For two states $x$ and $x’$ that differ only in the $i$-th coordinate, the update coordinate is given by the full conditional:</li> </ul> \[P(x→x′)=π(x'_{i}∣x_{−i})\] <p>It can be verified by:</p> \[π(x)π(x'_i∣x_{−i})=π(x')\] <p>which is consistent with the detailed balance requirement.</p> <p><strong>Practical considerations:</strong></p> <ul> <li> <strong>Consistency of Conditionals:</strong> For Gibbs sampling to work, the set of full conditionals must be consistent with a joint distribution $\pi(x)$. Under this condition, if the chain is run long enough, the joint distribution of the samples converges to $\pi(x)$.</li> <li> <p><strong>Detailed Balance in Gibbs Sampling:</strong> Although Gibbs sampling does not require an explicit acceptance step, one can show that it satisfies detailed balance. For two states $x$ and $x’$ that differ only in the $i$-th coordinate, the update probability is given by the full conditional:</p> <p>$P(x \to x’) = \pi\left(x_i’ \mid x_{-i}\right)$ It can be verified that:</p> <p>$\pi(x) \pi\left(x_i’ \mid x_{-i}\right) = \pi(x’)$ which is consistent with the detailed balance requirement.</p> </li> </ul> <p><strong>Convergence and ergodicity:</strong></p> <p>Gibbs sampling inherits the convergence properties of Markov chains. Provided that the chain is irreducible and aperiodic (often ensured by the structure of the conditional distributions), the Gibbs sampler is ergodic, meaning that the empirical averages converge to the true expectations under $\pi(x)$.</p> <p><strong>Practical considerations:</strong></p> <ul> <li> <strong>Blocking:</strong> In practice, it might be beneficial to update groups of variables together (blocked Gibbs sampling) when they are strongly correlated.</li> <li> <strong>Mixing:</strong> The rate at which the Gibbs sampler explores the state space (its mixing time) can be slow if the variables are highly correlated. In such cases, combining Gibbs updates with other moves (or using reparameterizations) may improve performance.</li> <li> <strong>Implementation:</strong> Gibbs sampling is particularly attractive when full conditionals are available in closed form (e.g., in many Bayesian hierarchical models).</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-mathematica highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">Initialize</span><span class="w"> </span><span class="nv">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="nv">x</span><span class="err">₁</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="err">₂</span><span class="o">,</span><span class="w"> </span><span class="err">…</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="nv">d</span><span class="p">)</span><span class="w">
</span><span class="nb">For</span><span class="w"> </span><span class="nv">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nb">N</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="w">
    </span><span class="nb">For</span><span class="w"> </span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">d</span><span class="o">:</span><span class="w">
        </span><span class="nv">Sample</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="nv">i</span><span class="o">^</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="o">_</span><span class="nv">i</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="nv">x</span><span class="err">₁</span><span class="o">^</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="err">…</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="p">(</span><span class="nv">i</span><span class="o">-</span><span class="m">1</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="o">_</span><span class="p">(</span><span class="nv">i</span><span class="o">+</span><span class="m">1</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="nv">t</span><span class="p">)</span><span class="o">,</span><span class="w"> </span><span class="err">…</span><span class="o">,</span><span class="w"> </span><span class="nv">x</span><span class="w">
        </span><span class="nv">d</span><span class="o">^</span><span class="p">(</span><span class="nv">t</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <h3 id="34-hamiltonian-monte-carlo-hmc">3.4. Hamiltonian Monte Carlo (HMC)</h3> <p><strong>Context &amp; Objective:</strong></p> <p>HMC is based on the Hamiltonian function:</p> \[H(x, p) = U(x) + K(p)\] <p>where:</p> <ul> <li>$U(x) = -\log \pi(x)$ is the <strong>potential energy</strong> (derived from the target density $\pi(x)$.</li> <li>$K(p) = \frac{1}{2}p^\top M^{-1}p$ is the <strong>kinetic energy</strong>, typically assuming $p \sim \mathcal{N}(0, M)$ with mass matrix $M$.</li> </ul> <p>Hamilton’s equations describe the evolution of $x$ and $p$:</p> <p>$\frac{dx}{dt} = \nabla_p H(x, p) = M^{-1}p, \qquad \frac{dp}{dt} = -\nabla_x H(x, p) = -\nabla U(x)$</p> <p><strong>Leapfrog integrator</strong></p> <p>In practice, Hamilton’s equations are solved numerically using the leapfrog integrator, which is chosen for its symplectic (volume-preserving) and time-reversible properties. The leapfrog update is performed in three steps:</p> <ol> <li><strong>Half-step momentum update:</strong></li> </ol> <p>$p\left(t + \frac{\epsilon}{2}\right) = p(t) - \frac{\epsilon}{2}\nabla U(x(t))$</p> <ol> <li><strong>Full-step position update:</strong></li> </ol> <p>$x(t + \epsilon) = x(t) + \epsilon\, M^{-1} p\left(t + \frac{\epsilon}{2}\right)$</p> <ol> <li><strong>Half-step momentum update:</strong></li> </ol> <p>$p(t + \epsilon) = p\left(t + \frac{\epsilon}{2}\right) - \frac{\epsilon}{2}\nabla U(x(t + \epsilon))$</p> <p>Repeating these steps for $L$ iterations produces a proposal $(x^<em>, p^</em>)$.</p> <p>Because numerical integration introduces discretization errors, HMC employs a Metropolis acceptance step to correct for these errors. The acceptance probability is given by:</p> \[\alpha = \min\left\{1, \exp\Big[-H(x^*, p^*) + H(x, p)\Big]\right\}\] <p>This step ensures that the overall transition kernel satisfies detailed balance with respect to the augmented target distribution $\pi(x) \, \mathcal{N}(p;0,M)$.</p> <p><strong>Convergence and efficiency:</strong></p> <ul> <li> <strong>Reduction of Random Walk Behavior:</strong> HMC can make large moves in state space while maintaining a high acceptance rate, thereby reducing autocorrelation.</li> <li> <strong>Tuning Parameters:</strong> The step size $\epsilon$ and the number of leapfrog steps $L$ must be carefully tuned. Too large a step size or too many steps can result in low acceptance probabilities, while too small values may result in inefficient exploration.</li> <li> <strong>Theoretical Guarantees:</strong> Under proper conditions (e.g. the leapfrog integrator’s error is bounded and the chain is irreducible and aperiodic), HMC is ergodic and converges to the target distribution.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">Initialize</span> <span class="nv">x</span><span class="err">₀</span>
<span class="nv">For</span> <span class="nv">t</span> <span class="o">=</span> <span class="mi">0</span> <span class="nv">to</span> <span class="nv">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nv">Sample</span> <span class="nv">momentum</span> <span class="nv">p</span> <span class="o">~</span> <span class="nv">N</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nv">M</span><span class="p">)</span>
    <span class="nv">Set</span> <span class="p">(</span><span class="nv">x</span><span class="p">,</span> <span class="nv">p</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="nv">x_t</span><span class="p">,</span> <span class="nv">p</span><span class="p">)</span>
    <span class="nv">Simulate</span> <span class="nv">Hamiltonian</span> <span class="nv">dynamics</span> <span class="nv">using</span> <span class="nv">the</span> <span class="nv">leapfrog</span> <span class="nv">integrator:</span>
        <span class="nv">For</span> <span class="nv">l</span> <span class="o">=</span> <span class="mi">1</span> <span class="nv">to</span> <span class="nv">L:</span>
            <span class="nv">p</span> <span class="o">=</span> <span class="nv">p</span> <span class="o">+</span> <span class="p">(</span><span class="err">ε</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="err">∇</span><span class="nb">log</span> <span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="p">)</span>
            <span class="nv">x</span> <span class="o">=</span> <span class="nv">x</span> <span class="o">+</span> <span class="err">ε</span> <span class="o">*</span> <span class="nv">M</span><span class="o">^</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="nv">p</span>
            <span class="nv">p</span> <span class="o">=</span> <span class="nv">p</span> <span class="o">+</span> <span class="p">(</span><span class="err">ε</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="err">∇</span><span class="nb">log</span> <span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="p">)</span>
    <span class="nv">Perform</span> <span class="nv">MH</span> <span class="nb">accept</span><span class="o">/</span><span class="nv">reject</span> <span class="nv">step</span> <span class="nv">with</span> <span class="nv">probability:</span>
        <span class="err">α</span> <span class="o">=</span> <span class="nv">min</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="nb">exp</span><span class="p">[</span> <span class="o">-</span><span class="nv">H</span><span class="p">(</span><span class="nv">x</span><span class="o">*</span><span class="p">,</span> <span class="nv">p</span><span class="o">*</span><span class="p">)</span> <span class="o">+</span> <span class="nv">H</span><span class="p">(</span><span class="nv">x_t</span><span class="p">,</span> <span class="nv">p</span><span class="p">)</span> <span class="p">]}</span>
    <span class="nv">If</span> <span class="nv">accepted:</span>
        <span class="nv">x_</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="nv">x</span><span class="o">*</span>
    <span class="nv">Else:</span>
        <span class="nv">x_</span><span class="p">(</span><span class="nv">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">=</span> <span class="nv">x_t</span>
</code></pre></div></div> <h3 id="35-metropolis-adjusted-langevin-algorithm-mala">3.5. Metropolis Adjusted Langevin Algorithm (MALA)</h3> <p><strong>Context &amp; Objective:</strong></p> <p>MALA enhances the standard Metropolis–Hastings algorithm by incorporating gradient information to propose moves that are more likely to be accepted. It is sometimes viewed as a discretized version of the Langevin diffusion process.</p> <p><strong>Langevin dynamics:</strong></p> <p>Consider the overdamped Langevin equation, which describes the evolution of $x$ in continuous time:</p> \[dx_t = \frac{1}{2}\nabla \log \pi(x_t) \, dt + dW_t\] <p>where $dW_t$ represents a Wiener process (or Brownian motion). The stationary distribution of this stochastic differential equation is $\pi(x)$.</p> <p><strong>Discretization and proposal:</strong></p> <p>Discretizing the Langevin equation with step size $\epsilon$ gives the proposal:</p> \[x' = x^{(t)} + \frac{\epsilon^2}{2}\nabla \log \pi(x^{(t)}) + \epsilon\, \eta, \quad \eta \sim \mathcal{N}(0, I)\] <p>This proposal is asymmetric due to the drift term $\frac{\epsilon^2}{2}\nabla \log \pi(x^{(t)})$.</p> <p><strong>Metropolis correction for MALA:</strong></p> <p>To correct for the discretization error and ensure that the chain converges to $\pi(x)$ an MH acceptance step is applied. The acceptance probability is computed as:</p> \[\alpha = \min\left\{1, \frac{\pi(x') \, q(x^{(t)} \mid x')}{\pi(x^{(t)}) \, q(x' \mid x^{(t)})}\right\}\] <p>where the proposal density $q$ is given by:</p> \[q(x' \mid x) = \mathcal{N}\left(x'; x + \frac{\epsilon^2}{2}\nabla \log \pi(x), \, \epsilon^2 I\right)\] <p><strong>Convergence and efficiency:</strong></p> <ul> <li> <strong>Incorporation of Gradient Information:</strong> The use of $\nabla \log \pi(x)$ helps propose moves that are informed by the geometry of the target distribution, often leading to a higher acceptance rate compared to random-walk proposals.</li> <li> <strong>Trade-off in Step Size:</strong> A small $\epsilon$ leads to high acceptance rates but slow exploration (small moves), while a large $\epsilon$ can improve exploration but may reduce the acceptance probability.</li> <li> <strong>Theoretical Guarantees:</strong> Under suitable conditions (including proper scaling of $\epsilon$ and the ergodicity of the underlying Langevin diffusion), MALA converges to $\pi(x)$.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-perl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">Initialize</span> <span class="nv">x</span><span class="err">₀</span>
<span class="nv">For</span> <span class="nv">t</span> <span class="o">=</span> <span class="mi">0</span> <span class="nv">to</span> <span class="nv">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nv">Compute</span> <span class="nv">gradient</span> <span class="nv">g</span> <span class="o">=</span> <span class="err">∇</span> <span class="nb">log</span> <span class="err">π</span><span class="p">(</span><span class="nv">x_t</span><span class="p">)</span>
    <span class="nv">Propose</span> <span class="nv">x</span><span class="p">'</span><span class="s1"> = x_t + (ε²/2)*g + ε*η, where η ~ N(0,I)
    Compute asymmetric proposal densities q(x</span><span class="p">'</span><span class="o">|</span><span class="nv">x_t</span><span class="p">)</span> <span class="ow">and</span> <span class="sx">q(x_t|x')</span>
    <span class="nv">Calculate</span> <span class="nv">acceptance</span> <span class="nv">probability:</span>
        <span class="err">α</span> <span class="o">=</span> <span class="nv">min</span><span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="err">π</span><span class="p">(</span><span class="nv">x</span><span class="p">'</span><span class="s1">)q(x_t|x</span><span class="p">')]</span> <span class="o">/</span> <span class="p">[</span><span class="err">π</span><span class="p">(</span><span class="nv">x_t</span><span class="p">)</span><span class="sx">q(x'|x_t)</span><span class="p">]</span> <span class="p">}</span>
    <span class="nv">Accept</span> <span class="ow">or</span> <span class="nv">reject</span> <span class="nv">accordingly</span><span class="o">.</span>
</code></pre></div></div> <h2 id="4-use-cases-implementations">4. Use cases implementations</h2> <p>Instead of implementing vanilla MCMC algorithms of toy examples, and retrieving simple distributions with libraries like Numpy and Scipy, we chose to implement these algorithms in the context of real world use cases, that range from computer vision to audio signal modeling.</p> <p>These implementations can be tweaked and used for solving real world problems.</p> <p>Here is a link to a Github repository where you can find the full implementations with some nice visualizations:</p> <p>https://github.com/Nizben/mcmc</p> <h3 id="41-bayesian-linear-regression-via-gibbs-sampling">4.1. Bayesian Linear Regression (via Gibbs Sampling)</h3> <p><strong>Context:</strong></p> <p>In Bayesian linear regression, we model the relationship:</p> \[y = X\beta + \epsilon,\quad \epsilon\sim\mathcal{N}(0,\sigma^2I)\] <p>with priors $\beta \sim \mathcal{N}(\mu_0, \Sigma_0)$ and $\sigma^2 \sim \text{Inv-Gamma}(\alpha_0, \beta_0)$. The Gibbs sampler alternates between sampling $\beta$ and $\sigma^2$ .</p> <p><strong>Python Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">invgamma</span><span class="p">,</span> <span class="n">multivariate_normal</span>

<span class="c1"># Generate synthetic data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">true_beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">true_beta</span> <span class="o">+</span> <span class="n">sigma_true</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Prior hyperparameters
</span><span class="n">beta_prior_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">beta_prior_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">alpha_prior</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">beta_prior_val</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Number of iterations for Gibbs sampling
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">beta_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">iterations</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">sigma2_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">iterations</span><span class="p">)</span>

<span class="c1"># Initial values
</span><span class="n">beta_current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">sigma2_current</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="c1"># Sample beta | sigma^2, y, X
</span>    <span class="n">V_beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta_prior_cov</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma2_current</span><span class="p">)</span>
    <span class="n">m_beta</span> <span class="o">=</span> <span class="n">V_beta</span> <span class="o">@</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta_prior_cov</span><span class="p">)</span> <span class="o">@</span> <span class="n">beta_prior_mean</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma2_current</span><span class="p">)</span>
    <span class="n">beta_current</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">m_beta</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">V_beta</span><span class="p">)</span>

    <span class="c1"># Sample sigma^2 | beta, y, X
</span>    <span class="n">alpha_post</span> <span class="o">=</span> <span class="n">alpha_prior</span> <span class="o">+</span> <span class="n">n</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_current</span>
    <span class="n">beta_post</span> <span class="o">=</span> <span class="n">beta_prior_val</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma2_current</span> <span class="o">=</span> <span class="n">invgamma</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">alpha_post</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta_post</span><span class="p">)</span>

    <span class="n">beta_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">beta_current</span>
    <span class="n">sigma2_samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma2_current</span>
</code></pre></div></div> <h3 id="42-audio-signal-reconstruction-with-metropolishastings-mh-and-preprocessing">4.2. Audio Signal Reconstruction with Metropolis–Hastings (MH) and Preprocessing</h3> <p><strong>Context:</strong></p> <p>Reconstructing a clean audio signal $s(t)$ from a noisy observation $y(t)$ involves preprocessing (e.g. filtering) and sampling from the posterior:</p> \[p(s|y) \propto p(y|s)p(s)\] <p>where $p(s)$ enforces smoothness.</p> <p><strong>Preprocessing &amp; MH Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.signal</span> <span class="kn">import</span> <span class="n">butter</span><span class="p">,</span> <span class="n">filtfilt</span>

<span class="c1"># Generate synthetic audio: sine wave with noise
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">s_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">s_true</span> <span class="o">+</span> <span class="n">noise_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>

<span class="c1"># --- Preprocessing ---
# Apply a low-pass Butterworth filter to remove high-frequency noise
</span><span class="k">def</span> <span class="nf">butter_lowpass_filter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">normal_cutoff</span> <span class="o">=</span> <span class="n">cutoff</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="nf">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">normal_cutoff</span><span class="p">,</span> <span class="n">btype</span><span class="o">=</span><span class="sh">'</span><span class="s">low</span><span class="sh">'</span><span class="p">,</span> <span class="n">analog</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">y_filtered</span> <span class="o">=</span> <span class="nf">filtfilt</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_filtered</span>

<span class="n">fs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Sampling frequency (Hz)
</span><span class="n">cutoff</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Cutoff frequency (Hz)
</span><span class="n">y_filtered</span> <span class="o">=</span> <span class="nf">butter_lowpass_filter</span><span class="p">(</span><span class="n">y_noisy</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">,</span> <span class="n">fs</span><span class="p">)</span>

<span class="c1"># --- Metropolis–Hastings for Signal Reconstruction ---
</span><span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">smoothness_prior</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lambda_reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">target</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">*</span> <span class="nf">smoothness_prior</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mh_audio_reconstruction</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">proposal_std</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">s_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>  <span class="c1"># Initialize with the preprocessed signal
</span>    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">s_proposal</span> <span class="o">=</span> <span class="n">s_current</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">s_current</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="nf">target</span><span class="p">(</span><span class="n">s_proposal</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">)</span> <span class="o">/</span> <span class="nf">target</span><span class="p">(</span><span class="n">s_current</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="nf">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ratio</span><span class="p">):</span>
            <span class="n">s_current</span> <span class="o">=</span> <span class="n">s_proposal</span>
        <span class="n">samples</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">s_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Run MH sampler on the preprocessed (filtered) signal
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">samples</span> <span class="o">=</span> <span class="nf">mh_audio_reconstruction</span><span class="p">(</span><span class="n">y_filtered</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="o">=</span><span class="n">noise_std</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">)</span>
<span class="n">s_reconstructed</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <h3 id="43-image-reconstruction-with-hamiltonian-monte-carlo-hmc">4.3. Image Reconstruction with Hamiltonian Monte Carlo (HMC)</h3> <p><strong>Context:</strong></p> <p>For image deblurring or reconstruction, consider the posterior:</p> \[p(I|Y) \propto p(Y|I)p(I)\] <p>where $I$ is the image, $Y$ is the observation, and $p(I)$ encodes spatial smoothness. HMC efficiently explores high-dimensional image spaces.</p> <p><strong>Simplified HMC Implementation (2D Image Denoising):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter</span>

<span class="c1"># Create synthetic image: gradient image with noise
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">image_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">true_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">image_size</span><span class="p">))</span>
<span class="n">noisy_image</span> <span class="o">=</span> <span class="n">true_image</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>

<span class="c1"># Define the target log-probability (negative energy) for the image.
# Combines a data fidelity term with a smoothness prior.
</span><span class="k">def</span> <span class="nf">log_target</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="n">fidelity</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">Y</span> <span class="o">-</span> <span class="n">I</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>
    <span class="c1"># Smoothness via a quadratic penalty on finite differences
</span>    <span class="n">smoothness</span> <span class="o">=</span> <span class="o">-</span><span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fidelity</span> <span class="o">+</span> <span class="n">smoothness</span>

<span class="c1"># HMC parameters
</span><span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">sigma_noise</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">lambda_reg</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Initialize with the noisy image
</span><span class="n">I_current</span> <span class="o">=</span> <span class="n">noisy_image</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">hmc_update</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">I_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">I</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">current_momentum</span> <span class="o">=</span> <span class="n">momentum</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="c1"># Compute gradient of log_target using finite differences (central differences)
</span>    <span class="k">def</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
        <span class="c1"># Fidelity term gradient
</span>        <span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>
        <span class="c1"># Smoothness gradient (using differences)
</span>        <span class="n">grad</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>
        <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="c1"># Leapfrog integration
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">I</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">momentum</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
        <span class="n">momentum</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="n">momentum</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="c1"># Negate momentum for symmetry
</span>    <span class="n">momentum</span> <span class="o">=</span> <span class="o">-</span><span class="n">momentum</span>

    <span class="c1"># Compute Hamiltonians
</span>    <span class="n">current_H</span> <span class="o">=</span> <span class="o">-</span><span class="nf">log_target</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">current_momentum</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">proposed_H</span> <span class="o">=</span> <span class="o">-</span><span class="nf">log_target</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">momentum</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">current_H</span> <span class="o">-</span> <span class="n">proposed_H</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">I</span><span class="p">,</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">I_current</span><span class="p">,</span> <span class="bp">False</span>

<span class="n">hmc_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accepted</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">I_new</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">hmc_update</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">noisy_image</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">acc</span><span class="p">:</span>
        <span class="n">accepted</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">I_current</span> <span class="o">=</span> <span class="n">I_new</span>
    <span class="n">hmc_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">I_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">HMC Acceptance Rate: </span><span class="si">{</span><span class="n">accepted</span><span class="o">/</span><span class="n">iterations</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <h3 id="44-implicit-neural-representations-neural-sdf-with-mala">4.4. Implicit Neural Representations (Neural SDF) with MALA</h3> <p><strong>Context:</strong></p> <p>Implicit neural representations (e.g., Neural Signed Distance Functions, SDFs) model continuous signals (e.g., 3D shapes) using neural networks. Uncertainty can be captured by placing a prior over latent variables. Here, we use MALA to sample from the posterior over a latent variable in a small Neural SDF model.</p> <p><strong>Improved Implementation &amp; Visualization:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define a simple Neural SDF with a latent vector parameter
</span><span class="k">class</span> <span class="nc">NeuralSDF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">NeuralSDF</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latent</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: [batch, 3]
</span>        <span class="n">latent_expand</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">latent_expand</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>

<span class="c1"># Generate simulated observations: points near a sphere of radius 0.8
</span><span class="k">def</span> <span class="nf">generate_sdf_observations</span><span class="p">(</span><span class="n">n_points</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># uniformly in [-1,1]^3
</span>    <span class="n">sdf_true</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.8</span>
    <span class="n">sdf_obs</span> <span class="o">=</span> <span class="n">sdf_true</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">sdf_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span>

<span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span> <span class="o">=</span> <span class="nf">generate_sdf_observations</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># Define log-likelihood and log-prior for the latent variable
</span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">sdf_obs</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">+</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># MALA update for the latent variable
</span><span class="k">def</span> <span class="nf">mala_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">logp</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">latent_current</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">latent_current</span><span class="p">)</span>
    <span class="n">latent_proposal</span> <span class="o">=</span> <span class="n">latent_current</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">step_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

    <span class="c1"># Compute acceptance probability (using symmetric proposal assumption)
</span>    <span class="n">latent_old</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">latent_proposal</span>
    <span class="n">logp_proposal</span> <span class="o">=</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">accept_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logp_proposal</span> <span class="o">-</span> <span class="n">logp</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">accept_prob</span><span class="p">:</span>
        <span class="n">accepted</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">latent_old</span>
        <span class="n">accepted</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="n">accepted</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">clone</span><span class="p">(),</span> <span class="n">logp_proposal</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">NeuralSDF</span><span class="p">(</span><span class="n">latent_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">latent_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accepts</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">accepted</span><span class="p">,</span> <span class="n">latent_sample</span><span class="p">,</span> <span class="n">lp</span> <span class="o">=</span> <span class="nf">mala_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">latent_samples</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latent_sample</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="n">log_probs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">lp</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">accepted</span><span class="p">:</span>
        <span class="n">accepts</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MALA Acceptance Rate: </span><span class="si">{</span><span class="n">accepts</span><span class="o">/</span><span class="n">iterations</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="5-conclusion">5. Conclusion</h2> <p>In this post, we have presented a rigorous exploration of MCMC methods. We began with theoretical foundations and then developed multiple algorithms with step-by-step pseudocode and theoretical justification. We detailed five major techniques: Importance Sampling, Metropolis–Hastings, Gibbs Sampling, Hamiltonian Monte Carlo, and MALA.</p> <p>The use cases further demonstrate the practicality of these methods:</p> <ul> <li> <strong>Bayesian linear regression</strong> uses Gibbs sampling to infer regression parameters.</li> <li> <strong>Audio signal reconstruction</strong> incorporates signal preprocessing before applying MH.</li> <li> <strong>Image reconstruction</strong> leverages HMC for efficient exploration in high dimensions.</li> <li> <strong>Implicit neural representations (Neural SDF)</strong> showcase MALA for sampling latent variables in modern deep learning models.</li> </ul> </body></html>
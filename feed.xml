<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nizben.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nizben.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-25T19:46:31+00:00</updated><id>https://nizben.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Accelerating 3D point cloud processing with a kernel-enhanced geometric transformer</title><link href="https://nizben.github.io/blog/2025/enhanced_transformer/" rel="alternate" type="text/html" title="Accelerating 3D point cloud processing with a kernel-enhanced geometric transformer"/><published>2025-02-26T00:00:00+00:00</published><updated>2025-02-26T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2025/enhanced_transformer</id><content type="html" xml:base="https://nizben.github.io/blog/2025/enhanced_transformer/"><![CDATA[<p>In this post, we present a new approach to 3D point cloud processing by combining kernel methods with geometric transformers. We integrate efficient Gaussian kernel computations (via <a href="http://kernel-operations.io"><strong>KeOps</strong></a>), scalable attention mechanisms (using <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention</a>), and custom CUDA kernels for neighborhood aggregation into a unified architecture. We cover theoretical motivations, detailed algorithmic derivations, low-level GPU optimizations, and different benchmarks to validate performance improvements over pure PyTorch implementations.</p> <h2 id="1-introduction"><strong>1. Introduction</strong></h2> <h3 id="11-background-and-motivation"><strong>1.1 Background and Motivation</strong></h3> <p>3D point clouds are fundamental to applications like autonomous navigation, robotics, and augmented reality. However, the challenges in processing large, sparse, and irregularly sampled data often lead to a trade-off between accuracy and computational efficiency. The issues include:</p> <ul> <li><strong>Scalability:</strong> Managing millions of points while preserving local geometric details.</li> <li><strong>Computational Bottlenecks:</strong> The quadratic cost of attention mechanisms and kernel computations.</li> <li><strong>Data Sparsity:</strong> Non-uniform sampling that makes local feature extraction difficult.</li> </ul> <p>To address these challenges, we propose a <strong>Kernel-Enhanced Geometric Transformer</strong> that combines three advanced techniques:</p> <ol> <li><strong>Efficient Kernel Computations with KeOps:</strong> Lazily evaluates large pairwise operations, avoiding memory explosion.</li> <li><strong>Flash Attention:</strong> Reduces the quadratic cost of standard self-attention by leveraging memory-efficient GPU kernels.</li> <li><strong>Custom CUDA Kernels:</strong> Optimizes neighborhood aggregation by directly exploiting GPU architectural features like shared memory and warp-level primitives.</li> </ol> <h3 id="12-contributions"><strong>1.2 Contributions</strong></h3> <p>Our work offers:</p> <ul> <li>A unified framework combining kernel-based similarity measures with transformer architectures to capture both global and local features.</li> <li>Enhanced computational efficiency through low-level GPU optimizations not achievable in pure PyTorch.</li> <li>A rigorous benchmarking protocol using CUDA events to quantify performance improvements.</li> </ul> <h2 id="3-the-math-of-this-transformer-architecture"><strong>3. The math of this transformer architecture</strong></h2> <h3 id="31-gaussian-kernel-similarity"><strong>3.1 Gaussian kernel similarity</strong></h3> <p>Given a set of 3D points ( $\mathbf{X} \in \mathbb{R}^{B \times N \times 3}$ ), the Gaussian kernel between points $\mathbf{x}_i$ and $\mathbf{x}_j$ is defined as:</p> \[K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{|\mathbf{x}_i - \mathbf{x}_j|^2}{2\sigma^2}\right)\] <p>This function emphasizes local relationships when $\sigma$ is small.</p> <h3 id="32-transformer-self-attention"><strong>3.2 Transformer self-attention</strong></h3> <p>The self-attention mechanism transforms an input sequence $\mathbf{X} \in \mathbb{R}^{N \times d}$ into queries ( $\mathbf{Q}$ ), keys ( $\mathbf{K}$ ), and values ( $\mathbf{V}$ ):</p> \[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}\] <p>Flash Attention refines this process to reduce memory consumption and computational complexity.</p> <h3 id="33-neighborhood-aggregation-via-cuda"><strong>3.3 Neighborhood aggregation via CUDA</strong></h3> <p>For each point $\mathbf{x}_i$ and its $K$ neighbors, we compute the aggregated feature as:</p> \[\mathbf{f}i = \frac{1}{K} \sum{k=1}^{K} \mathbf{g}(\mathbf{x}_{\text{neighbor}(i,k)})\] <p>Our custom CUDA kernel implements this mean aggregation with optimizations that are very hard to reproduce in pure PyTorch.</p> <h2 id="4-implementation-details"><strong>4. Implementation details</strong></h2> <h3 id="41-repository-structure"><strong>4.1 Repository structure</strong></h3> <p>The project is modularly structured:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kernel_enhanced_geometric_transformer/
├── cuda/
│   ├── neighborhood_aggregation.cu        # Optimized CUDA kernel
│   └── setup.py    # Build script for CUDA extension
├── models/
│   ├── __init__.py
│   ├── kernel_geometric_operations.py       # KeOps-based Gaussian kernel 
│   ├── flash_attention_geometric_transformer.py  # Flash Attention transformer
│   ├── custom_cuda_neighborhood.py          # Python wrapper for the CUDA kernel
│   └── enhanced_geometric_transformer.py    # Full model integration
├── datasets/
│   ├── __init__.py
│   └── custom_3d_dataset.py                 # Dataset loader and k-NN compute
├── train_geometric_transformer.py           # Training script
├── evaluate_geometric_transformer.py        # Evaluation script
├── benchmarks.py                            # Benchmarking scripts
├── requirements.txt                       
└── README.md                                
</code></pre></div></div> <h3 id="42-detailed-cuda-kernel-for-neighborhood-aggregation"><strong>4.2 Detailed CUDA kernel for neighborhood aggregation</strong></h3> <h3 id="421-enhanced-cuda-kernel-code"><strong>4.2.1 Enhanced CUDA kernel code</strong></h3> <p>Below is the enhanced CUDA kernel (in <code class="language-plaintext highlighter-rouge">cuda/neighborhood_aggregation.cu</code>) which uses shared memory, warp-level reductions, and loop unrolling:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;torch/extension.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span>
<span class="cp">#define THREADS_PER_BLOCK 256
#define MAX_NEIGHBORS 64// Assume K &lt;= MAX_NEIGHBORS for simplicity
</span>
<span class="c1">// Utility function for warp-level reduction (assumes 32 threads per warp)__inline__ __device__</span>

<span class="kt">float</span> <span class="nf">warpReduceSum</span><span class="p">(</span><span class="kt">float</span> <span class="n">val</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">warpSize</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">val</span> <span class="o">+=</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Enhanced CUDA kernel for neighborhood aggregation__global__ void enhanced_neighborhood_aggregation_kernel(</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">points</span><span class="p">,</span><span class="c1">// (B, N, C)const int* __restrict__ neighbors,// (B, N, K)float* __restrict__ aggregated,// (B, N, C)int B, int N, int K, int C)// Dimensions: Batch, Points, Neighbors, Channels</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">total</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">N</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">total</span><span class="p">)</span>
        <span class="k">return</span><span class="p">;</span>

    <span class="kt">int</span> <span class="n">b</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">N</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">N</span><span class="p">;</span>

<span class="c1">// Each thread processes one point; for each channel, we sum over K neighbors.for (int c = 0; c &lt; C; ++c) {</span>
        <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
        <span class="cp">#pragma unroll
</span>        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
            <span class="kt">int</span> <span class="n">neighbor_idx</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="n">b</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">K</span> <span class="o">+</span> <span class="n">k</span><span class="p">];</span>
            <span class="kt">float</span> <span class="n">val</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="n">b</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="n">neighbor_idx</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="n">c</span><span class="p">];</span>
            <span class="n">sum</span> <span class="o">+=</span> <span class="n">val</span><span class="p">;</span>
        <span class="p">}</span>
<span class="c1">// Demonstrate warp-level reduction (here each thread works independently,// but if collaborating across threads, such reduction would be used)float warp_sum = warpReduceSum(sum);</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">warpSize</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">sum</span> <span class="o">=</span> <span class="n">warp_sum</span><span class="p">;</span>
        <span class="n">aggregated</span><span class="p">[</span><span class="n">b</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">C</span> <span class="o">+</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span> <span class="o">/</span> <span class="kt">float</span><span class="p">(</span><span class="n">K</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Wrapper function exposed to Pythontorch::Tensor enhanced_neighborhood_aggregation(</span>
    <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">points</span><span class="p">,</span><span class="c1">// (B, N, C)</span>
    <span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">neighbors</span><span class="p">,</span><span class="c1">// (B, N, K)int C)// Number of channels</span>
<span class="p">{</span>
    <span class="k">auto</span> <span class="n">B</span> <span class="o">=</span> <span class="n">points</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">N</span> <span class="o">=</span> <span class="n">points</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">K</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
    <span class="k">auto</span> <span class="n">aggregated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">::</span><span class="n">zeros</span><span class="p">({</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">},</span> <span class="n">points</span><span class="p">.</span><span class="n">options</span><span class="p">());</span>

    <span class="kt">int</span> <span class="n">total</span> <span class="o">=</span> <span class="n">B</span> <span class="o">*</span> <span class="n">N</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">threads</span> <span class="o">=</span> <span class="n">THREADS_PER_BLOCK</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">total</span> <span class="o">+</span> <span class="n">threads</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">threads</span><span class="p">;</span>
    <span class="kt">size_t</span> <span class="n">sharedMemSize</span> <span class="o">=</span> <span class="n">threads</span> <span class="o">*</span> <span class="n">C</span> <span class="o">*</span> <span class="nf">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>

    <span class="n">enhanced_neighborhood_aggregation_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span> <span class="n">threads</span><span class="p">,</span> <span class="n">sharedMemSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
        <span class="n">points</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
        <span class="n">neighbors</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(),</span>
        <span class="n">aggregated</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">C</span>
    <span class="p">);</span>
    <span class="n">cudaError_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">cudaGetLastError</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">err</span> <span class="o">!=</span> <span class="n">cudaSuccess</span><span class="p">){</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">"CUDA Error: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">cudaGetErrorString</span><span class="p">(</span><span class="n">err</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">aggregated</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">"enhanced_neighborhood_aggregation"</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">enhanced_neighborhood_aggregation</span><span class="p">,</span> <span class="s">"Enhanced Neighborhood Aggregation CUDA Kernel"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="422-rationale-for-the-enhanced-kernel"><strong>4.2.2 Rationale for the enhanced kernel</strong></h3> <ul> <li> <p><strong>Shared memory and memory coalescing:</strong></p> <p>Although our example uses per-thread loops, further development could load blocks of neighbor features into shared memory to reduce global memory traffic—something not possible in high-level PyTorch.</p> </li> <li> <p><strong>Warp-level reduction:</strong></p> <p>The use of <code class="language-plaintext highlighter-rouge">__shfl_down_sync</code> allows fast intra-warp reductions, making summation over neighbors extremely efficient.</p> </li> <li> <p><strong>Loop unrolling:</strong></p> <p>The <code class="language-plaintext highlighter-rouge">#pragma unroll</code> directive lets the compiler optimize inner loops, further reducing overhead.</p> </li> </ul> <p>These optimizations are not accessible through PyTorch’s built-in functions (such as <code class="language-plaintext highlighter-rouge">torch.gather</code> and <code class="language-plaintext highlighter-rouge">torch.mean</code>), which do not offer low-level control over memory hierarchy or thread synchronization.</p> <h3 id="43-keops-based-gaussian-kernel-computations"><strong>4.3 KeOps-based gaussian kernel computations</strong></h3> <p>The module in <code class="language-plaintext highlighter-rouge">models/kernel_geometric_operations.py</code> uses KeOps LazyTensors to compute the Gaussian kernel similarity matrix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">pykeops.torch</span> <span class="kn">import</span> <span class="n">LazyTensor</span>

<span class="k">class</span> <span class="nc">KernelDistance</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">KernelDistance</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">points</span><span class="p">):</span>
				<span class="c1"># points: (B, N, 3)
</span>        <span class="n">X_i</span> <span class="o">=</span> <span class="nc">LazyTensor</span><span class="p">(</span><span class="n">points</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:])</span><span class="c1"># (B, N, 1, 3)
</span>        <span class="n">X_j</span> <span class="o">=</span> <span class="nc">LazyTensor</span><span class="p">(</span><span class="n">points</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span><span class="c1"># (B, 1, N, 3)
</span>        <span class="n">D_ij</span> <span class="o">=</span> <span class="p">((</span><span class="n">X_i</span> <span class="o">-</span> <span class="n">X_j</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="c1"># (B, N, N)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">D_ij</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)).</span><span class="nf">exp</span><span class="p">()</span><span class="c1"># (B, N, N) 
</span>        <span class="k">return</span> <span class="n">K</span>
</code></pre></div></div> <h2 id="5-rigorous-benchmarking-of-neighborhood-aggregation"><strong>5. Rigorous benchmarking of neighborhood aggregation</strong></h2> <p>To rigorously demonstrate the performance gap between a pure PyTorch implementation and our enhanced CUDA kernel, we use high-precision timing with CUDA events.</p> <h3 id="51-benchmarking-setup"><strong>5.1 Benchmarking setup</strong></h3> <p>The benchmarking protocol includes:</p> <ul> <li><strong>Warm-Up Runs:</strong> Initial iterations to avoid cold-start overhead.</li> <li><strong>CUDA Events for Timing:</strong> High-resolution timing using <code class="language-plaintext highlighter-rouge">torch.cuda.Event</code>.</li> <li><strong>Multiple Iterations:</strong> Averaging over many iterations (e.g., 100) to reduce noise.</li> <li><strong>Identical Workloads:</strong> Both implementations process the same input data for a fair comparison.</li> </ul> <h3 id="52-pure-pytorch-benchmark"><strong>5.2 Pure PyTorch benchmark</strong></h3> <p>A PyTorch-based neighborhood aggregation using <code class="language-plaintext highlighter-rouge">torch.gather</code> and <code class="language-plaintext highlighter-rouge">torch.mean</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">benchmark_pytorch</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Benchmark pure PyTorch neighborhood aggregation.
    - points: Tensor of shape (B, N, C)
    - neighbors: Tensor of shape (B, N, K)
    </span><span class="sh">"""</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">points</span><span class="p">.</span><span class="n">shape</span>
<span class="c1"># Warm-upfor _ in range(warmup):
</span>        <span class="n">neighbor_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="n">aggregated</span> <span class="o">=</span> <span class="n">neighbor_features</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

    <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">start_event</span><span class="p">.</span><span class="nf">record</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">neighbor_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="n">aggregated</span> <span class="o">=</span> <span class="n">neighbor_features</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">end_event</span><span class="p">.</span><span class="nf">record</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">start_event</span><span class="p">.</span><span class="nf">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="c1"># ms per iterationreturn elapsed_time
</span>
<span class="c1"># Example usage:
</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">16</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">neighbors</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">pure_pytorch_time</span> <span class="o">=</span> <span class="nf">benchmark_pytorch</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Pure PyTorch average time per iteration: {:.4f} ms</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">pure_pytorch_time</span><span class="p">))</span>

</code></pre></div></div> <h3 id="53-enhanced-cuda-kernel-benchmark"><strong>5.3 Enhanced CUDA kernel benchmark</strong></h3> <p>Benchmarking our custom CUDA kernel (assumed to be bound as <code class="language-plaintext highlighter-rouge">enhanced_neighborhood_aggregation</code>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">benchmark_cuda_kernel</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">custom_cuda_function</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Benchmark the custom CUDA kernel for neighborhood aggregation.
    - points: Tensor of shape (B, N, C)
    - neighbors: Tensor of shape (B, N, K)
    - custom_cuda_function: The CUDA kernel function (enhanced_neighborhood_aggregation)
    </span><span class="sh">"""</span>
<span class="c1"># Warm-upfor _ in range(warmup):
</span>        <span class="n">aggregated</span> <span class="o">=</span> <span class="nf">custom_cuda_function</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">points</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

    <span class="n">start_event</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">end_event</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Event</span><span class="p">(</span><span class="n">enable_timing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">start_event</span><span class="p">.</span><span class="nf">record</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="n">aggregated</span> <span class="o">=</span> <span class="nf">custom_cuda_function</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">points</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">end_event</span><span class="p">.</span><span class="nf">record</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>

    <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">start_event</span><span class="p">.</span><span class="nf">elapsed_time</span><span class="p">(</span><span class="n">end_event</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_iterations</span><span class="c1"># ms per iterationreturn elapsed_time
</span>
<span class="c1"># Example usage (assuming the CUDA function is imported):# from models.custom_cuda_neighborhood import enhanced_neighborhood_aggregation
</span><span class="n">cuda_kernel_time</span> <span class="o">=</span> <span class="nf">benchmark_cuda_kernel</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">custom_cuda_function</span><span class="o">=</span><span class="n">enhanced_neighborhood_aggregation</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Enhanced CUDA kernel average time per iteration: {:.4f} ms</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">cuda_kernel_time</span><span class="p">))</span>

<span class="c1"># Calculate speedup
</span><span class="n">speedup</span> <span class="o">=</span> <span class="n">pure_pytorch_time</span> <span class="o">/</span> <span class="n">cuda_kernel_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Speedup of CUDA kernel over PyTorch: {:.2f}x</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">speedup</span><span class="p">))</span>

</code></pre></div></div> <h3 id="54-complete-benchmarking-script"><strong>5.4 Complete benchmarking script</strong></h3> <p>You can combine both benchmarks in one script in <code class="language-plaintext highlighter-rouge">benchmarks.py</code> .</p> <h2 id="6-experimental-results"><strong>6. Experimental results</strong></h2> <p>After running the benchmarking suite:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pure PyTorch average <span class="nb">time </span>per iteration: 0.3123 ms
Enhanced CUDA kernel average <span class="nb">time </span>per iteration: 0.2189 ms
Output validation passed: Both implementations produce nearly identical results.
Speedup of CUDA kernel over PyTorch: 1.43x
</code></pre></div></div> <h2 id="7-repository-and-resources"><strong>7. Repository and Resources</strong></h2> <h3 id="71-github-repository"><strong>7.1 GitHub Repository</strong></h3> <p>Access the complete source code here:</p> <p><a href="https://github.com/nizben/Enhanced_transformer">Enhanced_transformer</a></p> <h3 id="72-setup-instructions"><strong>7.2 Setup Instructions</strong></h3> <ol> <li> <p><strong>Clone the Repository:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> git clone https://github.com/nizben/Enhanced_transformer.git
 <span class="nb">cd </span>Enhanced_transformer
</code></pre></div> </div> </li> <li> <p><strong>Install Dependencies:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div> </div> </li> <li> <p><strong>Build the CUDA Extension:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">cd </span>cuda
 python setup.py <span class="nb">install
 cd</span> ..
</code></pre></div> </div> </li> <li> <p><strong>Run Benchmarks:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> python benchmarks.py
</code></pre></div> </div> </li> </ol>]]></content><author><name></name></author><category term="Attention"/><category term="Cuda"/><summary type="html"><![CDATA[In this post, we present a new approach to 3D point cloud processing by combining kernel methods with geometric transformers. We integrate efficient Gaussian kernel computations (via KeOps), scalable attention mechanisms (using Flash Attention), and custom CUDA kernels for neighborhood aggregation into a unified architecture. We cover theoretical motivations, detailed algorithmic derivations, low-level GPU optimizations, and different benchmarks to validate performance improvements over pure PyTorch implementations.]]></summary></entry><entry><title type="html">Building an Autograd engine to understand Automatic Differentiation from the ground up</title><link href="https://nizben.github.io/blog/2025/demograd/" rel="alternate" type="text/html" title="Building an Autograd engine to understand Automatic Differentiation from the ground up"/><published>2025-02-03T00:00:00+00:00</published><updated>2025-02-03T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2025/demograd</id><content type="html" xml:base="https://nizben.github.io/blog/2025/demograd/"><![CDATA[<p><strong>Demograd</strong> is a minimal Autograd engine and neural network library built for educational purposes. It is designed to mimic the core functionalities of PyTorch, including a tensor class with automatic differentiation, a set of basic differentiable operations, activation functions, neural network layers, and optimizers. The design emphasizes clarity, modularity, and reproducibility.</p> <p>Check it out here :</p> <p><a href="https://github.com/nizben/demograd">GitHub - Nizben/demograd</a></p> <blockquote> <p>Note: This automatic differentiation engine is also heavily inspired by Karpathy’s <a href="https://github.com/karpathy/micrograd">Micrograd</a>.</p> </blockquote> <h2 id="overview"><strong>Overview</strong></h2> <p><strong>Demograd</strong> provides the following core components:</p> <ul> <li> <p><strong>Tensor and Autograd Engine:</strong></p> <p>The <code class="language-plaintext highlighter-rouge">Tensor</code> class (in <code class="language-plaintext highlighter-rouge">tensor_engine.py</code>) encapsulates NumPy arrays along with gradient information and a dependency graph. It supports automatic differentiation via a topological sorting mechanism for the computational graph.</p> </li> <li> <p><strong>Differentiable Operations:</strong></p> <p>A collection of basic operations (e.g., addition, subtraction, multiplication, division, exponentiation, logarithm, matrix multiplication) are implemented as subclasses of a base <code class="language-plaintext highlighter-rouge">Function</code> (in <code class="language-plaintext highlighter-rouge">functions.py</code>). Each operation defines a static <code class="language-plaintext highlighter-rouge">apply</code> method for the forward pass and a corresponding <code class="language-plaintext highlighter-rouge">backward</code> method for computing gradients.</p> </li> <li> <p><strong>Activation Functions:</strong></p> <p>Common activation functions such as ReLU, Sigmoid, Tanh, and Softmax are provided in <code class="language-plaintext highlighter-rouge">activations.py</code>. These functions follow the same autograd pattern, allowing them to be used seamlessly in network architectures.</p> </li> <li> <p><strong>Neural Network Layers:</strong></p> <p>A basic neural network module system is available in <code class="language-plaintext highlighter-rouge">nn.py</code>. This includes a <code class="language-plaintext highlighter-rouge">Linear</code> layer for fully connected networks and a <code class="language-plaintext highlighter-rouge">Sequential</code> container that aggregates multiple layers and collects their parameters.</p> </li> <li> <p><strong>Optimizers:</strong></p> <p>Simple optimizers (SGD and Adam) are implemented in <code class="language-plaintext highlighter-rouge">optimizers.py</code>. They operate on the parameters of the network and provide <code class="language-plaintext highlighter-rouge">step()</code> and <code class="language-plaintext highlighter-rouge">zero_grad()</code> methods to update weights based on computed gradients.</p> </li> <li> <p><strong>Example Training Script:</strong></p> <p>An example jupyter notebook (<code class="language-plaintext highlighter-rouge">example.ipynb</code>) demonstrates how to build and train a basic multilayer perceptron (MLP) on synthetic data using the provided modules.</p> </li> <li> <p><strong>Visualization:</strong> A computational graph building tool is also available in <code class="language-plaintext highlighter-rouge">visualization.py</code>. It provides the ability to visualize the computational graph of your modules and eases debugging and error tracking when building large networks.</p> </li> </ul> <h2 id="usage"><strong>Usage</strong></h2> <h3 id="building-models"><strong>Building Models</strong></h3> <p>You can construct neural networks by composing layers defined in <code class="language-plaintext highlighter-rouge">nn.py</code>. For example, a simple MLP can be created as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">demograd.nn</span> <span class="kn">import</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">demograd.activations</span> <span class="kn">import</span> <span class="n">ReLU</span>

<span class="c1"># Define an MLP with one hidden layer:
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">(</span>
    <span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
    <span class="n">ReLU</span><span class="p">.</span><span class="nb">apply</span><span class="p">,</span>
    <span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Autograd"/><summary type="html"><![CDATA[Demograd is a minimal Autograd engine and neural network library built for educational purposes. It is designed to mimic the core functionalities of PyTorch, including a tensor class with automatic differentiation, a set of basic differentiable operations, activation functions, neural network layers, and optimizers. The design emphasizes clarity, modularity, and reproducibility.]]></summary></entry><entry><title type="html">Understanding compiler and linker flags</title><link href="https://nizben.github.io/blog/2024/compiler/" rel="alternate" type="text/html" title="Understanding compiler and linker flags"/><published>2024-12-04T00:00:00+00:00</published><updated>2024-12-04T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2024/compiler</id><content type="html" xml:base="https://nizben.github.io/blog/2024/compiler/"><![CDATA[<p>When building a program in C, C++, or other compiled languages, one typically invokes a compiler or compiler driver (like <code class="language-plaintext highlighter-rouge">g++</code> or <code class="language-plaintext highlighter-rouge">clang</code>) with a list of command-line options. These options (often referred to as <strong>flags</strong>) tell the compiler how to compile (and sometimes link) your code.</p> <p>I stumbled upon many of these compiling/linking commands through my software engineering career, and this blogpost kind of reflects my current understanding of these concepts, from both a theoretical and practical viewpoints.</p> <p>In this post, we’ll cover:</p> <ul> <li>What flags are and how they’re used.</li> <li>The differences between compilation and linking.</li> <li>Detailed examples for CPU-only builds (GCC/Clang) and GPU-enabled builds (through CUDA’s <code class="language-plaintext highlighter-rouge">nvcc</code>for instance).</li> <li>Nuances like flag ordering, dual-phase flags, and platform-specific considerations.</li> </ul> <h2 id="1-what-are-flags">1. What Are “Flags”?</h2> <p>Compiler flags control different aspects of the compilation process.</p> <p>Typical flags include:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">I</code></strong>: Adds a directory to the compiler’s search path for header files.</li> <li><strong><code class="language-plaintext highlighter-rouge">L</code></strong>: Adds a directory to the linker’s search path for libraries.</li> <li><strong><code class="language-plaintext highlighter-rouge">l&lt;name&gt;</code></strong>: Links against a library named <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.so</code> (on Linux), <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.dylib</code> (on macOS), or <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.a</code> (static library).</li> <li><strong><code class="language-plaintext highlighter-rouge">O3</code>, <code class="language-plaintext highlighter-rouge">O2</code>, <code class="language-plaintext highlighter-rouge">O0</code></strong>: Controls compilation optimization levels (<code class="language-plaintext highlighter-rouge">O3</code> for advanced optimization, <code class="language-plaintext highlighter-rouge">O0</code> for none).</li> <li><strong><code class="language-plaintext highlighter-rouge">g</code></strong>: Generates debugging symbols for use with debuggers like <code class="language-plaintext highlighter-rouge">gdb</code>.</li> <li><strong><code class="language-plaintext highlighter-rouge">fPIC</code></strong>: Generates Position-Independent Code (required for shared libraries, e.g. <code class="language-plaintext highlighter-rouge">.so</code> files).</li> <li><strong><code class="language-plaintext highlighter-rouge">shared</code></strong>: Builds a shared library (e.g. <code class="language-plaintext highlighter-rouge">.so</code> on Linux).</li> <li><strong><code class="language-plaintext highlighter-rouge">std=c++11</code></strong> (or later standards like <code class="language-plaintext highlighter-rouge">c++14</code>, <code class="language-plaintext highlighter-rouge">c++17</code>): Specifies the C++ standard to use.</li> <li><strong><code class="language-plaintext highlighter-rouge">fopenmp</code></strong>: Enables OpenMP support for multithreading.</li> <li><strong><code class="language-plaintext highlighter-rouge">Xcompiler</code> or <code class="language-plaintext highlighter-rouge">Xlinker</code> (in <code class="language-plaintext highlighter-rouge">nvcc</code>)</strong>: Passes flags directly to the host compiler or linker.</li> </ul> <p>These flags provide precise control over how your code is parsed, optimized, and linked.</p> <h2 id="2-how-do-flags-relate-to-compilers">2. How Do Flags Relate to Compilers?</h2> <h3 id="21-compilation-vs-linking">2.1 Compilation vs. Linking</h3> <p><strong>Compilation</strong>:</p> <ul> <li>The compiler translates your source code (<code class="language-plaintext highlighter-rouge">.c</code>, <code class="language-plaintext highlighter-rouge">.cpp</code>, etc.) into object files (<code class="language-plaintext highlighter-rouge">.o</code>, <code class="language-plaintext highlighter-rouge">.obj</code>).</li> </ul> <p><strong>Linking</strong>:</p> <ul> <li>A linker (often invoked implicitly by the compiler driver) combines object files and required libraries into a final executable (e.g. <code class="language-plaintext highlighter-rouge">a.out</code>) or library (e.g. <code class="language-plaintext highlighter-rouge">.so</code>, <code class="language-plaintext highlighter-rouge">.dll</code>).</li> </ul> <p><strong>Examples of Flag Usage:</strong></p> <ul> <li><strong>Compilation-only flags</strong> (e.g., <code class="language-plaintext highlighter-rouge">I</code>, <code class="language-plaintext highlighter-rouge">std=c++11</code>, <code class="language-plaintext highlighter-rouge">fPIC</code>, <code class="language-plaintext highlighter-rouge">O3</code>) instruct the compiler on parsing, header search paths, optimizations, etc.</li> <li><strong>Linking-only flags</strong> (e.g., <code class="language-plaintext highlighter-rouge">L</code>, <code class="language-plaintext highlighter-rouge">l</code>) tell the linker where to find and which libraries to link against.</li> <li><strong>Dual-phase flags</strong>: Some flags like <code class="language-plaintext highlighter-rouge">fopenmp</code> affect both the compilation and linking stages, adding necessary libraries during linking.</li> </ul> <blockquote> <p>Note on Linker Flag Order:</p> <p>The order of libraries in the linker command can sometimes matter, particularly with static libraries. If unresolved symbols appear, adjusting the order might resolve them.</p> </blockquote> <h3 id="22-example-of-a-typical-g-command">2.2 Example of a Typical <code class="language-plaintext highlighter-rouge">g++</code> Command</h3> <p><strong>Compilation:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-I</span>/usr/local/include <span class="nt">-O3</span> <span class="nt">-std</span><span class="o">=</span>c++11 <span class="nt">-c</span> myfile.cpp <span class="nt">-o</span> myfile.o
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">I/usr/local/include</code>: Look for header files in <code class="language-plaintext highlighter-rouge">/usr/local/include</code>.</li> <li><code class="language-plaintext highlighter-rouge">O3</code> and <code class="language-plaintext highlighter-rouge">std=c++11</code>: Enable advanced optimization and specify the C++ standard.</li> <li><code class="language-plaintext highlighter-rouge">c</code>: Compile only (do not link). The output is <code class="language-plaintext highlighter-rouge">myfile.o</code>.</li> </ul> <p><strong>Linking:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ myfile.o <span class="nt">-L</span>/usr/local/lib <span class="nt">-lm</span> <span class="nt">-o</span> myapp
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">L/usr/local/lib</code>: Look for libraries in <code class="language-plaintext highlighter-rouge">/usr/local/lib</code>.</li> <li><code class="language-plaintext highlighter-rouge">lm</code>: Link against the math library (<code class="language-plaintext highlighter-rouge">libm.so</code> on Linux).</li> <li><code class="language-plaintext highlighter-rouge">o myapp</code>: Output an executable named <code class="language-plaintext highlighter-rouge">myapp</code>.</li> </ul> <p>You can also combine these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-I</span>/usr/local/include <span class="nt">-O3</span> <span class="nt">-std</span><span class="o">=</span>c++11 myfile.cpp <span class="nt">-L</span>/usr/local/lib <span class="nt">-lm</span> <span class="nt">-o</span> myapp
</code></pre></div></div> <p>Under the hood, the compiler first compiles, then links, all in one invocation.</p> <h2 id="3-what-are-linkers">3. What Are Linkers?</h2> <p>A <strong>linker</strong> is a program that resolves references between object files and external libraries. For example, if your code calls <code class="language-plaintext highlighter-rouge">sqrt()</code> from the math library, the linker locates where <code class="language-plaintext highlighter-rouge">sqrt</code> is defined (in <code class="language-plaintext highlighter-rouge">libm.so</code> on Linux, <code class="language-plaintext highlighter-rouge">.so</code> for <code class="language-plaintext highlighter-rouge">shared objects</code>) and links it into the final executable.</p> <p>When you pass flags like <code class="language-plaintext highlighter-rouge">-L/usr/local/lib -lm</code>, you’re telling the linker to:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">L/usr/local/lib</code></strong>: Search <code class="language-plaintext highlighter-rouge">/usr/local/lib</code> for libraries.</li> <li><strong><code class="language-plaintext highlighter-rouge">lm</code></strong>: Link against the math library (locating <code class="language-plaintext highlighter-rouge">libm.so</code>, <code class="language-plaintext highlighter-rouge">libm.a</code>, etc ….).</li> </ul> <h2 id="4-why-use-l-and-l">4. Why Use <code class="language-plaintext highlighter-rouge">L</code> and <code class="language-plaintext highlighter-rouge">l</code>?</h2> <ul> <li><strong><code class="language-plaintext highlighter-rouge">L&lt;dir&gt;</code></strong>: Adds <code class="language-plaintext highlighter-rouge">&lt;dir&gt;</code> to the linker’s search directories.</li> <li><strong><code class="language-plaintext highlighter-rouge">l&lt;name&gt;</code></strong>: Tells the linker to look for a file named <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.so</code>, <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.dylib</code>, or <code class="language-plaintext highlighter-rouge">lib&lt;name&gt;.a</code>.</li> </ul> <p><strong>Example:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ main.o <span class="nt">-L</span>/home/me/mylibs <span class="nt">-lfoo</span> <span class="nt">-o</span> main
</code></pre></div></div> <p>The linker checks <code class="language-plaintext highlighter-rouge">/home/me/mylibs</code> for <code class="language-plaintext highlighter-rouge">libfoo.so</code> (shared) or <code class="language-plaintext highlighter-rouge">libfoo.a</code> (static) and includes it in the final executable.</p> <blockquote> <p>Static vs. Shared Libraries:</p> <p>While the discussion above focuses on shared libraries (using flags like <code class="language-plaintext highlighter-rouge">-shared</code> and <code class="language-plaintext highlighter-rouge">-fPIC</code>), static libraries are linked differently. For static libraries, the linker includes the library code directly into the executable, and the flags used (<code class="language-plaintext highlighter-rouge">-l&lt;name&gt;</code>) point to <code class="language-plaintext highlighter-rouge">.a</code> files instead of <code class="language-plaintext highlighter-rouge">.so</code> or <code class="language-plaintext highlighter-rouge">.dylib</code>.</p> </blockquote> <h2 id="5-how-does-a-compiler-work-internally">5. How Does a Compiler Work Internally?</h2> <p>Here’s a very simplified overview of the compilation process:</p> <ol> <li><strong>Preprocessing</strong> <ul> <li>Processes directives like <code class="language-plaintext highlighter-rouge">#include</code> and <code class="language-plaintext highlighter-rouge">#define</code> (for code in C/C++ for instance), expanding macros and including header files.</li> </ul> </li> <li><strong>Parsing &amp; Semantic Analysis</strong> <ul> <li>Converts the source code into an internal representation (often an Abstract Syntax Tree) and checks for errors (e.g. type mismatches).</li> </ul> </li> <li><strong>Optimization</strong> <ul> <li>Applies optimizations based on flags such as <code class="language-plaintext highlighter-rouge">O2</code> or <code class="language-plaintext highlighter-rouge">O3</code>.</li> </ul> </li> <li><strong>Code Generation</strong> <ul> <li>Translates the optimized code into machine instructions (Assembly code).</li> </ul> </li> <li><strong>Assembly &amp; Object File Creation</strong> <ul> <li>The assembly is transformed into an object file (<code class="language-plaintext highlighter-rouge">.o</code> or <code class="language-plaintext highlighter-rouge">.obj</code>) containing machine code and unresolved symbols.</li> </ul> </li> <li><strong>Linking</strong> <ul> <li>The linker merges multiple object files and libraries to produce the final executable or library.</li> </ul> </li> </ol> <h2 id="6-code-snippets-of-typical-compilerlinker-commands">6. Code Snippets of Typical Compiler/Linker Commands</h2> <h3 id="61-c-on-linux-with-a-shared-library">6.1 C++ on Linux with a Shared Library</h3> <p><strong>Compile:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-fPIC</span> <span class="nt">-Iinclude</span> <span class="nt">-O3</span> <span class="nt">-std</span><span class="o">=</span>c++17 <span class="nt">-c</span> mylibrary.cpp <span class="nt">-o</span> mylibrary.o
</code></pre></div></div> <ul> <li><strong><code class="language-plaintext highlighter-rouge">fPIC</code></strong>: Required for generating position-independent code for shared libraries.</li> </ul> <p><strong>Link:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-shared</span> <span class="nt">-o</span> libmylibrary.so mylibrary.o
</code></pre></div></div> <ul> <li><strong><code class="language-plaintext highlighter-rouge">shared</code></strong>: Produces a <code class="language-plaintext highlighter-rouge">.so</code> shared object.</li> </ul> <p><strong>Using the Shared Library:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ main.cpp <span class="nt">-Iinclude</span> <span class="nt">-L</span><span class="nb">.</span> <span class="nt">-lmylibrary</span> <span class="nt">-o</span> main
</code></pre></div></div> <ul> <li><strong><code class="language-plaintext highlighter-rouge">Iinclude</code></strong>: Look for header files.</li> <li><strong><code class="language-plaintext highlighter-rouge">L.</code></strong>: Look in the current directory for libraries.</li> <li><strong><code class="language-plaintext highlighter-rouge">lmylibrary</code></strong>: Link against <code class="language-plaintext highlighter-rouge">libmylibrary.so</code>.</li> </ul> <hr/> <h3 id="62-using-openmp">6.2 Using OpenMP</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>g++ <span class="nt">-fopenmp</span> <span class="nt">-O3</span> main.cpp <span class="nt">-o</span> main
</code></pre></div></div> <ul> <li><strong><code class="language-plaintext highlighter-rouge">fopenmp</code></strong>: Enables both compiler and linker support for OpenMP, automatically linking with the OpenMP runtime (e.g., <code class="language-plaintext highlighter-rouge">libgomp</code>).</li> </ul> <hr/> <h3 id="63-example-with-clang-on-macos">6.3 Example with Clang on macOS</h3> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clang++ <span class="nt">-std</span><span class="o">=</span>c++14 <span class="nt">-stdlib</span><span class="o">=</span>libc++ <span class="nt">-I</span>/usr/local/include <span class="nt">-L</span>/usr/local/lib main.cpp <span class="nt">-o</span> myapp <span class="nt">-lc</span>++
</code></pre></div></div> <ul> <li>macOS often defaults to different standard libraries (<code class="language-plaintext highlighter-rouge">libc++</code> vs. <code class="language-plaintext highlighter-rouge">libstdc++</code>), so specifying <code class="language-plaintext highlighter-rouge">stdlib=libc++</code> and linking with <code class="language-plaintext highlighter-rouge">lc++</code> is useful.</li> <li><strong>Platform differences:</strong> Although this post focuses on Linux and macOS, note that Windows uses different conventions (such as <code class="language-plaintext highlighter-rouge">.lib</code> files).</li> </ul> <h2 id="7-cuda-example">7. CUDA Example</h2> <p>When compiling CUDA code, you typically use <code class="language-plaintext highlighter-rouge">nvcc</code>, NVIDIA’s compiler driver. Under the hood, <code class="language-plaintext highlighter-rouge">nvcc</code> calls the host compiler (like <code class="language-plaintext highlighter-rouge">gcc</code> or <code class="language-plaintext highlighter-rouge">clang</code>) for CPU code.</p> <p><strong>Basic CUDA Compilation:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc <span class="nt">-I</span>/path/to/cuda/include <span class="nt">-c</span> mykernel.cu <span class="nt">-o</span> mykernel.o
</code></pre></div></div> <p><strong>More Complex Example:</strong></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc <span class="nt">-arch</span><span class="o">=</span>sm_75 <span class="se">\\</span>
     <span class="nt">-I</span>/usr/local/cuda/include <span class="se">\\</span>
     <span class="nt">-I</span>/usr/local/include <span class="se">\\</span>
     <span class="nt">-L</span>/usr/local/cuda/lib64 <span class="se">\\</span>
     <span class="nt">-lcudart</span> <span class="se">\\</span>
     <span class="nt">-Xcompiler</span> <span class="nt">-fPIC</span> <span class="se">\\</span>
     <span class="nt">-O3</span> <span class="nt">-o</span> mycudaapp main.cu
</code></pre></div></div> <ul> <li><strong><code class="language-plaintext highlighter-rouge">arch=sm_75</code></strong>: Specifies the target GPU architecture (e.g., NVIDIA Turing).</li> <li><strong><code class="language-plaintext highlighter-rouge">I/usr/local/cuda/include</code></strong>: Specifies the directory for CUDA headers (e.g <code class="language-plaintext highlighter-rouge">cuda_runtime.h</code>).</li> <li><strong><code class="language-plaintext highlighter-rouge">L/usr/local/cuda/lib64</code> &amp; <code class="language-plaintext highlighter-rouge">lcudart</code></strong>: Directs the linker to the CUDA runtime library (<code class="language-plaintext highlighter-rouge">libcudart.so</code>).</li> <li><strong><code class="language-plaintext highlighter-rouge">Xcompiler -fPIC</code></strong>: Passes <code class="language-plaintext highlighter-rouge">fPIC</code> to the host compiler.</li> <li><strong><code class="language-plaintext highlighter-rouge">O3</code></strong>: Applies optimization.</li> </ul> <h2 id="8-putting-it-all-together">8. Putting It All Together</h2> <ul> <li> <p><strong>Headers (<code class="language-plaintext highlighter-rouge">.h</code>, <code class="language-plaintext highlighter-rouge">.hpp</code>)</strong></p> <p>Found via <code class="language-plaintext highlighter-rouge">-I</code> paths.</p> </li> <li> <p><strong>Libraries (<code class="language-plaintext highlighter-rouge">libsomething.so</code>, <code class="language-plaintext highlighter-rouge">libsomething.a</code>)</strong></p> <p>Found via <code class="language-plaintext highlighter-rouge">-L</code> paths and included with <code class="language-plaintext highlighter-rouge">-lsomething</code>.</p> </li> <li> <p><strong>Optimization and Standards</strong></p> <p>Controlled via flags like <code class="language-plaintext highlighter-rouge">-O2</code>, <code class="language-plaintext highlighter-rouge">-O3</code>, and <code class="language-plaintext highlighter-rouge">-std=c++17</code>.</p> </li> <li> <p><strong>Position-Independent Code (PIC)</strong></p> <p>Required for shared libraries via <code class="language-plaintext highlighter-rouge">-fPIC</code>.</p> </li> <li> <p><strong>Shared Library Output</strong></p> <p>Created with <code class="language-plaintext highlighter-rouge">-shared</code>.</p> </li> <li> <p><strong>OpenMP</strong></p> <p>Enabled with <code class="language-plaintext highlighter-rouge">-fopenmp</code>, with consideration for dual-phase (compilation and linking) behavior.</p> </li> <li> <p><strong>CUDA</strong></p> <p>Managed by <code class="language-plaintext highlighter-rouge">nvcc</code> alongside host compiler flags, targeting specific GPU architectures with <code class="language-plaintext highlighter-rouge">-arch=sm_xx</code>.</p> </li> </ul> <h2 id="9-summary">9. Summary</h2> <p>Understanding compiler and linker flags is crucial for building functional and optimized software. Here’s what you need to remember:</p> <ul> <li><strong>Compiler flags</strong> shape how the source code is processed, optimized, and debugged.</li> <li><strong>Linker flags</strong> (<code class="language-plaintext highlighter-rouge">L</code> and <code class="language-plaintext highlighter-rouge">l</code>) determine how external libraries are located and included.</li> <li>A <strong>linker</strong> resolves external symbols by combining object files and libraries.</li> <li><strong>CUDA builds</strong> require managing both device and host code, with <code class="language-plaintext highlighter-rouge">nvcc</code> orchestrating the process.</li> <li><strong>Platform Nuances &amp; Ordering:</strong> Pay attention to the order of linker flags and be aware of platform-specific differences (Linux, macOS, Windows).</li> </ul> <p>By carefully controlling these flags, you can ensure your project is both robust and efficient.</p>]]></content><author><name></name></author><category term="Compiler"/><category term="Linker"/><category term="Flags"/><summary type="html"><![CDATA[When building a program in C, C++, or other compiled languages, one typically invokes a compiler or compiler driver (like g++ or clang) with a list of command-line options. These options (often referred to as flags) tell the compiler how to compile (and sometimes link) your code.]]></summary></entry><entry><title type="html">Making sense of Score, Flow, and Diffusion models</title><link href="https://nizben.github.io/blog/2024/score/" rel="alternate" type="text/html" title="Making sense of Score, Flow, and Diffusion models"/><published>2024-12-04T00:00:00+00:00</published><updated>2024-12-04T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2024/score</id><content type="html" xml:base="https://nizben.github.io/blog/2024/score/"><![CDATA[<p>This post is an attempt to bridge the gap between different ideas around the latest techniques in generative modeling. We will try to do it in a mathematically rigorous fashion, and meticulously unpacking the theory and the links between these models.</p> <p>Throughout, we use the following notations:</p> <ul> <li>$\mathbf{x} \in \mathbb{R}^d$ denotes data (or a random variable in the data space).</li> <li>$p(\mathbf{x})$ denotes the data distribution.</li> <li>$\pi(\mathbf{z})$ typically denotes a base (or prior) distribution in a latent space $\mathbf{z}\in \mathbb{R}^d$. A common choice is $\pi(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{I})$.</li> <li>For time-dependent distributions, we write $q_t(\mathbf{x})$ or $p_t(\mathbf{x})$.</li> <li>$\nabla_{\mathbf{x}}$ denotes the gradient operator w.r.t. $\mathbf{x}$.</li> <li>$\nabla_{\mathbf{x}} \cdot (\cdot)$ denotes the divergence operator w.r.t. $\mathbf{x}$.</li> </ul> <h2 id="introduction-to-generative-modeling">Introduction to generative modeling</h2> <p>A <strong>generative model</strong> is a parameterized family of probability distributions $p_{\theta}(\mathbf{x})$ that we seek to match to a true data distribution $p_{\text{data}}(\mathbf{x})$. One typically has <strong>i.i.d.</strong> samples from $p_{\text{data}}$ (the training data). We want to:</p> <ol> <li><strong>Train</strong> $p_{\theta}(\mathbf{x})$ so that $p_{\theta}\approx p_{\text{data}}$.</li> <li><strong>Generate (sample)</strong> new data $\mathbf{x}$ from $p_{\theta}$.</li> <li>Potentially <strong>evaluate</strong> or compare densities for model-based reasoning.</li> </ol> <p>Different generative modeling paradigms include:</p> <ul> <li><strong>Normalizing flows</strong> (explicitly invertible mappings or continuous-time analogs).</li> <li><strong>Variational Autoencoders (VAEs)</strong> (encoder-decoder with latent variables).</li> <li><strong>Generative Adversarial Networks (GANs)</strong> (adversarial training).</li> <li><strong>Energy-Based Models</strong> (unnormalized densities).</li> <li><strong>Score-Based / Diffusion Models</strong> (using a forward noising process and reverse-time score estimation).</li> </ul> <p>In this post, we will focus on:</p> <ul> <li><strong>Flow matching</strong>: A continuous-time method to learn velocity fields that morph one distribution into another.</li> <li><strong>Score matching</strong>: A technique to learn the gradient of a log-density function.</li> <li><strong>Diffusion models</strong>: A special case of (time-dependent) score-matching that uses an SDE to degrade data and a reverse SDE to generate.</li> </ul> <h2 id="flow-based-generative-modeling">Flow-based generative modeling</h2> <h3 id="traditional-normalizing-flows">Traditional normalizing flows</h3> <p>In a <em>discrete</em> normalizing flow, one designs a sequence of <em>invertible</em> mappings $ f_i: \mathbb{R}^d \to \mathbb{R}^d$, $i=1,\dots,L$. Denote the base distribution $\pi(\mathbf{z})$, often $\mathcal{N}(\mathbf{0},\mathbf{I})$. A sample from the model is constructed as:</p> \[\mathbf{z}_0 \sim \pi(\mathbf{z}), \quad \mathbf{z}_1 = f_1(\mathbf{z}_0), \quad \mathbf{z}_2 = f_2(\mathbf{z}_1), \quad \dots \quad \mathbf{z}_L = f_L(\mathbf{z}_{L-1}) =: \mathbf{x}.\] <p>Hence $\mathbf{x} \sim p_{\theta}(\mathbf{x})$. If each $f_i$ is invertible, the model distribution can be <strong>exactly</strong> expressed:</p> \[p_{\theta}(\mathbf{x}) = \pi \bigl(f^{-1}(\mathbf{x})\bigr)\, \left\lvert \det \nabla_{\mathbf{x}} f^{-1}(\mathbf{x}) \right\rvert,\] <p>where $f = f_L \circ \dots \circ f_1$. Training typically maximizes the log-likelihood $\log p_{\theta}(\mathbf{x})$ over data $\mathbf{x}$. But carefully designing invertible $f_i$ with tractable Jacobian determinants can be restrictive.</p>]]></content><author><name></name></author><category term="Score"/><category term="Flow"/><category term="Diffusion"/><summary type="html"><![CDATA[This post is an attempt to bridge the gap between different ideas around the latest techniques in generative modeling. We will try to do it in a mathematically rigorous fashion, and meticulously unpacking the theory and the links between these models.]]></summary></entry><entry><title type="html">Introduction to MCMC methods: From importance sampling to advanced algorithms</title><link href="https://nizben.github.io/blog/2024/mcmc/" rel="alternate" type="text/html" title="Introduction to MCMC methods: From importance sampling to advanced algorithms"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2024/mcmc</id><content type="html" xml:base="https://nizben.github.io/blog/2024/mcmc/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <p>This post (or rather a collection of notes) is an attempt to go through different concepts around <strong>MCMC methods</strong> from the ground up. I will be trying to gather and structure my learnings around this topic in a way that is clear, intelligible, and beginner friendly.</p> <p>Sampling from complex, high-dimensional probability distributions is a fundamental challenge in statistics, machine learning, and many applied fields. The core idea is that Markov Chain Monte Carlo (MCMC) methods overcome this challenge by constructing a Markov chain whose stationary distribution is the target distribution.</p> <p>In practice, MCMC is used for Bayesian inference, uncertainty quantification, and solving inverse problems in areas such as audio and image processing. This blogpost proposes an accessible introduction to MCMC methods from its theoretical foundations to some Python implementations of these algorithms for real world use cases.</p> <h2 id="2-theoretical-foundations">2. Theoretical Foundations</h2> <h3 id="21-markov-chains-and-stationarity">2.1. Markov Chains and Stationarity</h3> <p>A <strong>Markov chain</strong> is a sequence of random variables ${X_t}_{t \geq 0}$ that satisfies the Markov property:</p> \[P(X_{t+1} = y \mid X_t = x, X_{t-1} = x_{t-1}, \dots) = P(X_{t+1}=y \mid X_t=x).\] <p>The chain’s dynamics are defined by a <strong>transition probability</strong> $P(x,y)$ (or kernel) that is time-independent.</p> <p>A probability distribution $\pi(x)$ is said to be <strong>stationary</strong> (or invariant) if, for all states $y$,</p> \[\pi(y) = \sum_{x} \pi(x) P(x, y)\] <p>This means that if : $X_0 \sim \pi, \text{ then } X_t \sim \pi \text{ for every } t.$</p> <h3 id="22-detailed-balance-and-invariance">2.2. Detailed Balance and Invariance</h3> <p>A sufficient condition for stationarity is the <strong>detailed balance condition</strong>:</p> \[\pi(x) P(x, y) = \pi(y) P(y, x), \quad \forall x, y\] <p><strong>Proof Sketch:</strong></p> <ol> <li><strong>Assume detailed balance:</strong> For every pair $(x,y)$:</li> </ol> \[\pi(x) P(x, y) = \pi(y) P(y, x)\] <ol> <li><strong>Sum over all $x$ for a fixed $y$:</strong></li> </ol> \[\sum_{x} \pi(x) P(x, y) = \sum_{x} \pi(y) P(y, x) = \pi(y) \sum_{x} P(y, x)\] <ol> <li><strong>Normalization:</strong> Since $\sum_{x} P(y, x) = 1$, we obtain:</li> </ol> \[\sum_{x} \pi(x) P(x, y) = \pi(y)\] <p>Thus, detailed balance guarantees that $\pi$ is invariant under the chain dynamics.</p> <h3 id="23-convergence-and-ergodicity">2.3. Convergence and Ergodicity</h3> <p>For the empirical averages from the chain to converge to expectations under $\pi$, the chain must be:</p> <ul> <li><strong>Irreducible:</strong> Every state can be reached from any other state.</li> <li><strong>Aperiodic:</strong> The chain does not get trapped in cycles.</li> <li><strong>Positive Recurrent:</strong> The expected return time to any state is finite.</li> </ul> <p>If these conditions hold, the <strong>ergodic theorem</strong> asserts that for any integrable function $f$:</p> \[\frac{1}{N}\sum_{t=1}^{N} f(X_t) \longrightarrow \mathbb{E}_{\pi}[f(x)]\] <p>as $N \to \infty$.</p> <p>This is the foundation behind using MCMC to approximate integrals and expectations.</p> <h2 id="3-mcmc-algorithms">3. MCMC Algorithms</h2> <p>Below are several common MCMC algorithms with some theroretical details, context, and pseudocode.</p> <h3 id="31-importance-sampling">3.1. Importance Sampling</h3> <p><strong>Context &amp; Objective:</strong></p> <p>Often, our goal is to compute expectations under a complex target distribution $\pi(x)$. For example, we may wish to evaluate:</p> \[I = \int f(x)\pi(x)\,dx\] <p>When direct sampling from $\pi(x)$ is infeasible, we introduce a proposal (or importance) distribution $q(x)$ that is easier to sample from.</p> <p><strong>Theory &amp; Formulation:</strong></p> <p>Given samples $x_1, x_2, \dots, x_N$ drawn from $q(x)$, the expectation is estimated as:</p> \[I\approx \frac{\sum_{i=1}^{N} f(x_i) w(x_i)}{\sum_{i=1}^{N} w(x_i)},\] <p>with importance weights defined by:</p> \[w(x) = \frac{\pi(x)}{q(x)}\] <p><em>Key Considerations:</em></p> <ul> <li><strong>Choice of $q(x):q(x)$</strong> must have heavier tails than $\pi(x)$ to avoid extremely high weights.</li> <li><strong>Variance:</strong> A poor choice of $q(x)$ leads to high variance in estimates.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For i = 1 to N:
    Sample x_i ~ q(x)
    Compute weight w_i = π(x_i) / q(x_i)
Estimate I ≈ (Σ f(x_i) w_i) / (Σ w_i)
</code></pre></div></div> <h3 id="32-metropolishastings-mh">3.2. Metropolis–Hastings (MH)</h3> <p><strong>Context &amp; Objective:</strong></p> <p>MH constructs a Markov chain whose stationary distribution is $\pi(x)$. It proposes moves using a proposal density $q(x’|x)$ and accepts these moves with a carefully designed acceptance probability. The key idea is to design the transition probability $P(x \to x’)$ so that the target distribution is invariant. A sufficient condition is <strong>detailed balance</strong>, which states that for all states $x$ and $x’$:</p> <p>$\pi(x) P(x \to x’) = \pi(x’) P(x’ \to x)$</p> <p>In MH, the transition probability is given by:</p> \[P(x \to x') = q(x'|x) \alpha(x, x')\] <p>where $\alpha(x, x’)$ is the acceptance probability. To satisfy detailed balance, we require:</p> \[\pi(x) q(x'|x) \alpha(x, x') = \pi(x') q(x|x') \alpha(x', x)\] <p>A common and effective choice is to define $\alpha(x, x’)$ as:</p> \[\alpha(x, x') = \min\left\{1, \frac{\pi(x') \, q(x|x')}{\pi(x) \, q(x'|x)}\right\}\] <table> <tbody> <tr> <td>When $$\pi(x’)q(x</td> <td>x’) \geq \pi(x)q(x’</td> <td>x)$$, we have $\alpha(x, x’) = 1$, otherwise, the move is accepted with probability:</td> </tr> </tbody> </table> \[\frac{\pi(x') \, q(x|x')}{\pi(x) \, q(x'|x)}\] <p><strong>Convergence and ergodicity</strong></p> <p>The MH algorithm constructs a Markov chain that, under suitable conditions (irreducibility, aperiodicity, and positive recurrence), converges to the target distribution $\pi(x)$. The ergodic theorem then guarantees that time averages computed from the chain will converge to the expectations under $\pi(x)$:</p> \[\frac{1}{N}\sum_{t=1}^{N} f(x_t) \longrightarrow \mathbb{E}_{\pi}[f(x)] \quad \text{as } N \to \infty\] <p><strong>Practical considerations:</strong></p> <ul> <li> <table> <tbody> <tr> <td><strong>Choice of Proposal:</strong> The efficiency of the MH algorithm heavily depends on how well the proposal distribution explores the state space. If $q(x’</td> <td>x)$ is too narrow, the chain will explore slowly, if too wide, the acceptance rate may drop.</td> </tr> </tbody> </table> </li> <li><strong>Symmetric Proposals:</strong> When $q(x’|x) = q(x|x’)$ (as in a Gaussian random walk), the acceptance probability simplifies to: \(\alpha(x, x') = \min\left\{1, \frac{\pi(x')}{\pi(x)}\right\}\)</li> </ul> <p><strong>Algorithm &amp; Pseudocode:</strong></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize x₀
For t = 0 to N - 1:
    Propose x' ~ q(x'|x_t)
    Calculate acceptance probability:
        α = min{1, [π(x') q(x_t|x')] / [π(x_t) q(x'|x_t)] }
    With probability α:
        Set x_(t+1) = x'
    Else:
        Set x_(t+1) = x_t
</code></pre></div></div> <h3 id="33-gibbs-sampling">3.3. Gibbs Sampling</h3> <p><strong>Context &amp; Objective:</strong></p> <p>Gibbs sampling is a special case of the Metropolis–Hastings algorithm, optimized for high-dimensional problems where the joint distribution $\pi(x)$ is difficult to sample from directly, but the full conditional distributions $π(x_i∣x_{−i})$ (where $x_{-i}$ denotes all components except $x_i$) are tractable.</p> <p>Suppose $x = (x_1, x_2, \dots, x_d)$ is a $d$-dimensional vector. The Gibbs sampler iterates over each coordinate and updates it by sampling from the full conditional distribution:</p> \[x_i^{(t+1)} \sim \pi\left(x_i \mid x_1^{(t+1)}, \dots, x_{i-1}^{(t+1)}, x_{i+1}^{(t)}, \dots, x_d^{(t)}\right)\] <p>Since each update is drawn exactly from the full conditional, the move is automatically accepted. The chain is constructed to have $\pi(x)$ as its stationary distribution.</p> <p><strong>Theoretical details:</strong></p> <ul> <li><strong>Consistency of conditionals:</strong> For Gibbs sampling to work, the set of full conditionals must be consistent with a joint distribution $\pi(x)$. Under this condition, if the chain is run long enough the joint ditribution of the samples converges to $\pi(x)$.</li> <li><strong>Detailed balance in Gibbs sampling:</strong> Although Gibbs sampling does not require an explicit acceptance step, one can show that it satisfies detailed balance. For two states $x$ and $x’$ that differ only in the $i$-th coordinate, the update coordinate is given by the full conditional:</li> </ul> \[P(x→x′)=π(x'_{i}∣x_{−i})\] <p>It can be verified by:</p> \[π(x)π(x'_i∣x_{−i})=π(x')\] <p>which is consistent with the detailed balance requirement.</p> <p><strong>Practical considerations:</strong></p> <ul> <li><strong>Consistency of Conditionals:</strong> For Gibbs sampling to work, the set of full conditionals must be consistent with a joint distribution $\pi(x)$. Under this condition, if the chain is run long enough, the joint distribution of the samples converges to $\pi(x)$.</li> <li> <p><strong>Detailed Balance in Gibbs Sampling:</strong> Although Gibbs sampling does not require an explicit acceptance step, one can show that it satisfies detailed balance. For two states $x$ and $x’$ that differ only in the $i$-th coordinate, the update probability is given by the full conditional:</p> <p>$P(x \to x’) = \pi\left(x_i’ \mid x_{-i}\right)$ It can be verified that:</p> <p>$\pi(x) \pi\left(x_i’ \mid x_{-i}\right) = \pi(x’)$ which is consistent with the detailed balance requirement.</p> </li> </ul> <p><strong>Convergence and ergodicity:</strong></p> <p>Gibbs sampling inherits the convergence properties of Markov chains. Provided that the chain is irreducible and aperiodic (often ensured by the structure of the conditional distributions), the Gibbs sampler is ergodic, meaning that the empirical averages converge to the true expectations under $\pi(x)$.</p> <p><strong>Practical considerations:</strong></p> <ul> <li><strong>Blocking:</strong> In practice, it might be beneficial to update groups of variables together (blocked Gibbs sampling) when they are strongly correlated.</li> <li><strong>Mixing:</strong> The rate at which the Gibbs sampler explores the state space (its mixing time) can be slow if the variables are highly correlated. In such cases, combining Gibbs updates with other moves (or using reparameterizations) may improve performance.</li> <li><strong>Implementation:</strong> Gibbs sampling is particularly attractive when full conditionals are available in closed form (e.g., in many Bayesian hierarchical models).</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize x = (x₁, x₂, …, x_d)
For t = 0 to N - 1:
    For i = 1 to d:
        Sample x_i^(t+1) ~ π(x_i | x₁^(t+1), …, x_(i-1)^(t+1), x_(i+1)^(t), …, x
        d^(t))
</code></pre></div></div> <h3 id="34-hamiltonian-monte-carlo-hmc">3.4. Hamiltonian Monte Carlo (HMC)</h3> <p><strong>Context &amp; Objective:</strong></p> <p>HMC is based on the Hamiltonian function:</p> \[H(x, p) = U(x) + K(p)\] <p>where:</p> <ul> <li>$U(x) = -\log \pi(x)$ is the <strong>potential energy</strong> (derived from the target density $\pi(x)$.</li> <li>$K(p) = \frac{1}{2}p^\top M^{-1}p$ is the <strong>kinetic energy</strong>, typically assuming $p \sim \mathcal{N}(0, M)$ with mass matrix $M$.</li> </ul> <p>Hamilton’s equations describe the evolution of $x$ and $p$:</p> <p>$\frac{dx}{dt} = \nabla_p H(x, p) = M^{-1}p, \qquad \frac{dp}{dt} = -\nabla_x H(x, p) = -\nabla U(x)$</p> <p><strong>Leapfrog integrator</strong></p> <p>In practice, Hamilton’s equations are solved numerically using the leapfrog integrator, which is chosen for its symplectic (volume-preserving) and time-reversible properties. The leapfrog update is performed in three steps:</p> <ol> <li><strong>Half-step momentum update:</strong></li> </ol> <p>$p\left(t + \frac{\epsilon}{2}\right) = p(t) - \frac{\epsilon}{2}\nabla U(x(t))$</p> <ol> <li><strong>Full-step position update:</strong></li> </ol> <p>$x(t + \epsilon) = x(t) + \epsilon\, M^{-1} p\left(t + \frac{\epsilon}{2}\right)$</p> <ol> <li><strong>Half-step momentum update:</strong></li> </ol> <p>$p(t + \epsilon) = p\left(t + \frac{\epsilon}{2}\right) - \frac{\epsilon}{2}\nabla U(x(t + \epsilon))$</p> <p>Repeating these steps for $L$ iterations produces a proposal $(x^<em>, p^</em>)$.</p> <p>Because numerical integration introduces discretization errors, HMC employs a Metropolis acceptance step to correct for these errors. The acceptance probability is given by:</p> \[\alpha = \min\left\{1, \exp\Big[-H(x^*, p^*) + H(x, p)\Big]\right\}\] <p>This step ensures that the overall transition kernel satisfies detailed balance with respect to the augmented target distribution $\pi(x) \, \mathcal{N}(p;0,M)$.</p> <p><strong>Convergence and efficiency:</strong></p> <ul> <li><strong>Reduction of Random Walk Behavior:</strong> HMC can make large moves in state space while maintaining a high acceptance rate, thereby reducing autocorrelation.</li> <li><strong>Tuning Parameters:</strong> The step size $\epsilon$ and the number of leapfrog steps $L$ must be carefully tuned. Too large a step size or too many steps can result in low acceptance probabilities, while too small values may result in inefficient exploration.</li> <li><strong>Theoretical Guarantees:</strong> Under proper conditions (e.g. the leapfrog integrator’s error is bounded and the chain is irreducible and aperiodic), HMC is ergodic and converges to the target distribution.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize x₀
For t = 0 to N - 1:
    Sample momentum p ~ N(0, M)
    Set (x, p) = (x_t, p)
    Simulate Hamiltonian dynamics using the leapfrog integrator:
        For l = 1 to L:
            p = p + (ε/2)*∇log π(x)
            x = x + ε * M^(-1) * p
            p = p + (ε/2)*∇log π(x)
    Perform MH accept/reject step with probability:
        α = min{1, exp[ -H(x*, p*) + H(x_t, p) ]}
    If accepted:
        x_(t+1) = x*
    Else:
        x_(t+1) = x_t
</code></pre></div></div> <h3 id="35-metropolis-adjusted-langevin-algorithm-mala">3.5. Metropolis Adjusted Langevin Algorithm (MALA)</h3> <p><strong>Context &amp; Objective:</strong></p> <p>MALA enhances the standard Metropolis–Hastings algorithm by incorporating gradient information to propose moves that are more likely to be accepted. It is sometimes viewed as a discretized version of the Langevin diffusion process.</p> <p><strong>Langevin dynamics:</strong></p> <p>Consider the overdamped Langevin equation, which describes the evolution of $x$ in continuous time:</p> \[dx_t = \frac{1}{2}\nabla \log \pi(x_t) \, dt + dW_t\] <p>where $dW_t$ represents a Wiener process (or Brownian motion). The stationary distribution of this stochastic differential equation is $\pi(x)$.</p> <p><strong>Discretization and proposal:</strong></p> <p>Discretizing the Langevin equation with step size $\epsilon$ gives the proposal:</p> \[x' = x^{(t)} + \frac{\epsilon^2}{2}\nabla \log \pi(x^{(t)}) + \epsilon\, \eta, \quad \eta \sim \mathcal{N}(0, I)\] <p>This proposal is asymmetric due to the drift term $\frac{\epsilon^2}{2}\nabla \log \pi(x^{(t)})$.</p> <p><strong>Metropolis correction for MALA:</strong></p> <p>To correct for the discretization error and ensure that the chain converges to $\pi(x)$ an MH acceptance step is applied. The acceptance probability is computed as:</p> \[\alpha = \min\left\{1, \frac{\pi(x') \, q(x^{(t)} \mid x')}{\pi(x^{(t)}) \, q(x' \mid x^{(t)})}\right\}\] <p>where the proposal density $q$ is given by:</p> \[q(x' \mid x) = \mathcal{N}\left(x'; x + \frac{\epsilon^2}{2}\nabla \log \pi(x), \, \epsilon^2 I\right)\] <p><strong>Convergence and efficiency:</strong></p> <ul> <li><strong>Incorporation of Gradient Information:</strong> The use of $\nabla \log \pi(x)$ helps propose moves that are informed by the geometry of the target distribution, often leading to a higher acceptance rate compared to random-walk proposals.</li> <li><strong>Trade-off in Step Size:</strong> A small $\epsilon$ leads to high acceptance rates but slow exploration (small moves), while a large $\epsilon$ can improve exploration but may reduce the acceptance probability.</li> <li><strong>Theoretical Guarantees:</strong> Under suitable conditions (including proper scaling of $\epsilon$ and the ergodicity of the underlying Langevin diffusion), MALA converges to $\pi(x)$.</li> </ul> <p><strong>Pseudocode:</strong></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initialize x₀
For t = 0 to N - 1:
    Compute gradient g = ∇ log π(x_t)
    Propose x' = x_t + (ε²/2)*g + ε*η, where η ~ N(0,I)
    Compute asymmetric proposal densities q(x'|x_t) and q(x_t|x')
    Calculate acceptance probability:
        α = min{1, [π(x')q(x_t|x')] / [π(x_t)q(x'|x_t)] }
    Accept or reject accordingly.
</code></pre></div></div> <h2 id="4-use-cases-implementations">4. Use cases implementations</h2> <p>Instead of implementing vanilla MCMC algorithms of toy examples, and retrieving simple distributions with libraries like Numpy and Scipy, we chose to implement these algorithms in the context of real world use cases, that range from computer vision to audio signal modeling.</p> <p>These implementations can be tweaked and used for solving real world problems.</p> <p>Here is a link to a Github repository where you can find the full implementations with some nice visualizations:</p> <p><a href="https://github.com/Nizben/mcmc">MCMC</a></p> <h3 id="41-bayesian-linear-regression-via-gibbs-sampling">4.1. Bayesian Linear Regression (via Gibbs Sampling)</h3> <p><strong>Context:</strong></p> <p>In Bayesian linear regression, we model the relationship:</p> \[y = X\beta + \epsilon,\quad \epsilon\sim\mathcal{N}(0,\sigma^2I)\] <p>with priors $\beta \sim \mathcal{N}(\mu_0, \Sigma_0)$ and $\sigma^2 \sim \text{Inv-Gamma}(\alpha_0, \beta_0)$. The Gibbs sampler alternates between sampling $\beta$ and $\sigma^2$ .</p> <p><strong>Python Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">invgamma</span><span class="p">,</span> <span class="n">multivariate_normal</span>

<span class="c1"># Generate synthetic data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">n</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">true_beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">sigma_true</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">true_beta</span> <span class="o">+</span> <span class="n">sigma_true</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># Prior hyperparameters
</span><span class="n">beta_prior_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">beta_prior_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
<span class="n">alpha_prior</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">beta_prior_val</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Number of iterations for Gibbs sampling
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">beta_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">iterations</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
<span class="n">sigma2_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">iterations</span><span class="p">)</span>

<span class="c1"># Initial values
</span><span class="n">beta_current</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">sigma2_current</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="c1"># Sample beta | sigma^2, y, X
</span>    <span class="n">V_beta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta_prior_cov</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma2_current</span><span class="p">)</span>
    <span class="n">m_beta</span> <span class="o">=</span> <span class="n">V_beta</span> <span class="o">@</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta_prior_cov</span><span class="p">)</span> <span class="o">@</span> <span class="n">beta_prior_mean</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma2_current</span><span class="p">)</span>
    <span class="n">beta_current</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">m_beta</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">V_beta</span><span class="p">)</span>

    <span class="c1"># Sample sigma^2 | beta, y, X
</span>    <span class="n">alpha_post</span> <span class="o">=</span> <span class="n">alpha_prior</span> <span class="o">+</span> <span class="n">n</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta_current</span>
    <span class="n">beta_post</span> <span class="o">=</span> <span class="n">beta_prior_val</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">residuals</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma2_current</span> <span class="o">=</span> <span class="n">invgamma</span><span class="p">.</span><span class="nf">rvs</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">alpha_post</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">beta_post</span><span class="p">)</span>

    <span class="n">beta_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">beta_current</span>
    <span class="n">sigma2_samples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma2_current</span>
</code></pre></div></div> <h3 id="42-audio-signal-reconstruction-with-metropolishastings-mh-and-preprocessing">4.2. Audio Signal Reconstruction with Metropolis–Hastings (MH) and Preprocessing</h3> <p><strong>Context:</strong></p> <p>Reconstructing a clean audio signal $s(t)$ from a noisy observation $y(t)$ involves preprocessing (e.g. filtering) and sampling from the posterior:</p> \[p(s|y) \propto p(y|s)p(s)\] <p>where $p(s)$ enforces smoothness.</p> <p><strong>Preprocessing &amp; MH Implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.signal</span> <span class="kn">import</span> <span class="n">butter</span><span class="p">,</span> <span class="n">filtfilt</span>

<span class="c1"># Generate synthetic audio: sine wave with noise
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">s_true</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>
<span class="n">noise_std</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">s_true</span> <span class="o">+</span> <span class="n">noise_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>

<span class="c1"># --- Preprocessing ---
# Apply a low-pass Butterworth filter to remove high-frequency noise
</span><span class="k">def</span> <span class="nf">butter_lowpass_filter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">,</span> <span class="n">fs</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">nyq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">fs</span>
    <span class="n">normal_cutoff</span> <span class="o">=</span> <span class="n">cutoff</span> <span class="o">/</span> <span class="n">nyq</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="nf">butter</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">normal_cutoff</span><span class="p">,</span> <span class="n">btype</span><span class="o">=</span><span class="sh">'</span><span class="s">low</span><span class="sh">'</span><span class="p">,</span> <span class="n">analog</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">y_filtered</span> <span class="o">=</span> <span class="nf">filtfilt</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y_filtered</span>

<span class="n">fs</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Sampling frequency (Hz)
</span><span class="n">cutoff</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Cutoff frequency (Hz)
</span><span class="n">y_filtered</span> <span class="o">=</span> <span class="nf">butter_lowpass_filter</span><span class="p">(</span><span class="n">y_noisy</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">,</span> <span class="n">fs</span><span class="p">)</span>

<span class="c1"># --- Metropolis–Hastings for Signal Reconstruction ---
</span><span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">smoothness_prior</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lambda_reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">target</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">*</span> <span class="nf">smoothness_prior</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mh_audio_reconstruction</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">proposal_std</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">s_current</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>  <span class="c1"># Initialize with the preprocessed signal
</span>    <span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">s_proposal</span> <span class="o">=</span> <span class="n">s_current</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">proposal_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">s_current</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="nf">target</span><span class="p">(</span><span class="n">s_proposal</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">)</span> <span class="o">/</span> <span class="nf">target</span><span class="p">(</span><span class="n">s_current</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="nf">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ratio</span><span class="p">):</span>
            <span class="n">s_current</span> <span class="o">=</span> <span class="n">s_proposal</span>
        <span class="n">samples</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">s_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Run MH sampler on the preprocessed (filtered) signal
</span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">samples</span> <span class="o">=</span> <span class="nf">mh_audio_reconstruction</span><span class="p">(</span><span class="n">y_filtered</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="o">=</span><span class="n">noise_std</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">)</span>
<span class="n">s_reconstructed</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div> <h3 id="43-image-reconstruction-with-hamiltonian-monte-carlo-hmc">4.3. Image Reconstruction with Hamiltonian Monte Carlo (HMC)</h3> <p><strong>Context:</strong></p> <p>For image deblurring or reconstruction, consider the posterior:</p> \[p(I|Y) \propto p(Y|I)p(I)\] <p>where $I$ is the image, $Y$ is the observation, and $p(I)$ encodes spatial smoothness. HMC efficiently explores high-dimensional image spaces.</p> <p><strong>Simplified HMC Implementation (2D Image Denoising):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.ndimage</span> <span class="kn">import</span> <span class="n">gaussian_filter</span>

<span class="c1"># Create synthetic image: gradient image with noise
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">image_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">true_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">image_size</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">image_size</span><span class="p">))</span>
<span class="n">noisy_image</span> <span class="o">=</span> <span class="n">true_image</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">)</span>

<span class="c1"># Define the target log-probability (negative energy) for the image.
# Combines a data fidelity term with a smoothness prior.
</span><span class="k">def</span> <span class="nf">log_target</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="n">fidelity</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">Y</span> <span class="o">-</span> <span class="n">I</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>
    <span class="c1"># Smoothness via a quadratic penalty on finite differences
</span>    <span class="n">smoothness</span> <span class="o">=</span> <span class="o">-</span><span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fidelity</span> <span class="o">+</span> <span class="n">smoothness</span>

<span class="c1"># HMC parameters
</span><span class="n">step_size</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">sigma_noise</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">lambda_reg</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Initialize with the noisy image
</span><span class="n">I_current</span> <span class="o">=</span> <span class="n">noisy_image</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">hmc_update</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">):</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">I_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="o">*</span><span class="n">I</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">current_momentum</span> <span class="o">=</span> <span class="n">momentum</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

    <span class="c1"># Compute gradient of log_target using finite differences (central differences)
</span>    <span class="k">def</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
        <span class="c1"># Fidelity term gradient
</span>        <span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>
        <span class="c1"># Smoothness gradient (using differences)
</span>        <span class="n">grad</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>
        <span class="n">grad</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:])</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="n">grad</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="o">-=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">lambda_reg</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">I</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="c1"># Leapfrog integration
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">I</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">momentum</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="nf">grad_log_target</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
        <span class="n">momentum</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="n">momentum</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="c1"># Negate momentum for symmetry
</span>    <span class="n">momentum</span> <span class="o">=</span> <span class="o">-</span><span class="n">momentum</span>

    <span class="c1"># Compute Hamiltonians
</span>    <span class="n">current_H</span> <span class="o">=</span> <span class="o">-</span><span class="nf">log_target</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">current_momentum</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">proposed_H</span> <span class="o">=</span> <span class="o">-</span><span class="nf">log_target</span><span class="p">(</span><span class="n">I</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">momentum</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">current_H</span> <span class="o">-</span> <span class="n">proposed_H</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">I</span><span class="p">,</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">I_current</span><span class="p">,</span> <span class="bp">False</span>

<span class="n">hmc_images</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accepted</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">I_new</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">hmc_update</span><span class="p">(</span><span class="n">I_current</span><span class="p">,</span> <span class="n">noisy_image</span><span class="p">,</span> <span class="n">step_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">sigma_noise</span><span class="p">,</span> <span class="n">lambda_reg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">acc</span><span class="p">:</span>
        <span class="n">accepted</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">I_current</span> <span class="o">=</span> <span class="n">I_new</span>
    <span class="n">hmc_images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">I_current</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">HMC Acceptance Rate: </span><span class="si">{</span><span class="n">accepted</span><span class="o">/</span><span class="n">iterations</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <h3 id="44-implicit-neural-representations-neural-sdf-with-mala">4.4. Implicit Neural Representations (Neural SDF) with MALA</h3> <p><strong>Context:</strong></p> <p>Implicit neural representations (e.g., Neural Signed Distance Functions, SDFs) model continuous signals (e.g., 3D shapes) using neural networks. Uncertainty can be captured by placing a prior over latent variables. Here, we use MALA to sample from the posterior over a latent variable in a small Neural SDF model.</p> <p><strong>Improved Implementation &amp; Visualization:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define a simple Neural SDF with a latent vector parameter
</span><span class="k">class</span> <span class="nc">NeuralSDF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">NeuralSDF</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latent</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: [batch, 3]
</span>        <span class="n">latent_expand</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">latent_expand</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>

<span class="c1"># Generate simulated observations: points near a sphere of radius 0.8
</span><span class="k">def</span> <span class="nf">generate_sdf_observations</span><span class="p">(</span><span class="n">n_points</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">n_points</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># uniformly in [-1,1]^3
</span>    <span class="n">sdf_true</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.8</span>
    <span class="n">sdf_obs</span> <span class="o">=</span> <span class="n">sdf_true</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">sdf_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span>

<span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span> <span class="o">=</span> <span class="nf">generate_sdf_observations</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>

<span class="c1"># Define log-likelihood and log-prior for the latent variable
</span><span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">sdf_obs</span> <span class="o">-</span> <span class="n">pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">+</span> <span class="nf">log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># MALA update for the latent variable
</span><span class="k">def</span> <span class="nf">mala_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">logp</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
    <span class="n">latent_current</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">latent_current</span><span class="p">)</span>
    <span class="n">latent_proposal</span> <span class="o">=</span> <span class="n">latent_current</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">step_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">noise</span>

    <span class="c1"># Compute acceptance probability (using symmetric proposal assumption)
</span>    <span class="n">latent_old</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">latent_proposal</span>
    <span class="n">logp_proposal</span> <span class="o">=</span> <span class="nf">log_posterior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">accept_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logp_proposal</span> <span class="o">-</span> <span class="n">logp</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">accept_prob</span><span class="p">:</span>
        <span class="n">accepted</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">latent_old</span>
        <span class="n">accepted</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="n">accepted</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">latent</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">clone</span><span class="p">(),</span> <span class="n">logp_proposal</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">NeuralSDF</span><span class="p">(</span><span class="n">latent_dim</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">latent_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">accepts</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">accepted</span><span class="p">,</span> <span class="n">latent_sample</span><span class="p">,</span> <span class="n">lp</span> <span class="o">=</span> <span class="nf">mala_update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">sdf_obs</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">latent_samples</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">latent_sample</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="n">log_probs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">lp</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">accepted</span><span class="p">:</span>
        <span class="n">accepts</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MALA Acceptance Rate: </span><span class="si">{</span><span class="n">accepts</span><span class="o">/</span><span class="n">iterations</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="5-conclusion">5. Conclusion</h2> <p>In this post, we have presented a rigorous exploration of MCMC methods. We began with theoretical foundations and then developed multiple algorithms with step-by-step pseudocode and theoretical justification. We detailed five major techniques: Importance Sampling, Metropolis–Hastings, Gibbs Sampling, Hamiltonian Monte Carlo, and MALA.</p> <p>The use cases further demonstrate the practicality of these methods:</p> <ul> <li><strong>Bayesian linear regression</strong> uses Gibbs sampling to infer regression parameters.</li> <li><strong>Audio signal reconstruction</strong> incorporates signal preprocessing before applying MH.</li> <li><strong>Image reconstruction</strong> leverages HMC for efficient exploration in high dimensions.</li> <li><strong>Implicit neural representations (Neural SDF)</strong> showcase MALA for sampling latent variables in modern deep learning models.</li> </ul>]]></content><author><name></name></author><category term="MCMC"/><summary type="html"><![CDATA[1. Introduction]]></summary></entry><entry><title type="html">Image and video quality quantitative metrics</title><link href="https://nizben.github.io/blog/2024/fid/" rel="alternate" type="text/html" title="Image and video quality quantitative metrics"/><published>2024-08-28T00:00:00+00:00</published><updated>2024-08-28T00:00:00+00:00</updated><id>https://nizben.github.io/blog/2024/fid</id><content type="html" xml:base="https://nizben.github.io/blog/2024/fid/"><![CDATA[<p>These are some notes with some code around FID, SSIM, and PSNR metrics.</p> <p>Github repository for the full jupyter notebook:</p> <p><a href="https://github.com/Nizben/Image_Video_metrics">Image and Video Metrics</a></p> <h3 id="1-fréchet-inception-distance-fid-for-videos"><strong>1. Fréchet Inception Distance (FID) for videos</strong></h3> <h3 id="mathematical-intuition"><strong>Mathematical intuition</strong></h3> <p>FID measures the similarity between two sets of videos by comparing their feature distributions using the Fréchet distance between multivariate Gaussians. For videos, features are extracted using a <strong>3D CNN</strong> to capture spatiotemporal patterns.</p> <p><strong>Formula</strong>:</p> \[\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}\left(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2}\right)\] <ul> <li> <table> <tbody> <tr> <td><strong>Mean distance</strong>: $</td> <td> </td> <td>\mu_r - \mu_g</td> <td> </td> <td>^2$ reflects how close the average features of real $\mu_r$ and generated $\mu_g$ videos are.</td> </tr> </tbody> </table> </li> <li><strong>Covariance alignment</strong>: The trace term ${Tr}(\cdot)$ quantifies how well the feature variations $(\Sigma_r, \Sigma_g)$ match.</li> </ul> <h3 id="implementation-with-a-pretrained-3d-cnn"><strong>Implementation with a pretrained 3D CNN</strong></h3> <p>We use <strong>I3D (Inflated 3D ConvNet)</strong> pretrained on Kinetics-400 for feature extraction (pretty standard).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="n">pytorchvideo.models.hub</span> <span class="kn">import</span> <span class="n">i3d_r50</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">class</span> <span class="nc">VideoFeatureExtractor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nf">i3d_r50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">videos</span><span class="p">):</span>
        <span class="c1"># videos: (B, C, T, H, W), pixel values in [0, 255]
</span>        <span class="n">videos</span> <span class="o">=</span> <span class="n">videos</span> <span class="o">/</span> <span class="mf">255.0</span>  <span class="c1"># Normalize to [0, 1]
</span>
        <span class="c1"># Resize spatial dimensions using Option 1:
</span>        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">videos</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">videos</span> <span class="o">=</span> <span class="n">videos</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
        <span class="n">videos</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">videos</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">bilinear</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">videos</span> <span class="o">=</span> <span class="n">videos</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        
        <span class="n">features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">videos</span><span class="p">)</span>  <span class="c1"># Output: (B, 2048)
</span>        <span class="k">return</span> <span class="n">features</span>

<span class="k">def</span> <span class="nf">calculate_fid</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">generated_features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">real_features</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">generated_features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">generated_features</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">sigma1</span> <span class="o">+=</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">sigma1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">sigma2</span> <span class="o">+=</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">sigma2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">covmean</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">linalg</span><span class="p">.</span><span class="nf">sqrtm</span><span class="p">(</span><span class="n">sigma1</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">sigma2</span><span class="p">),</span> <span class="n">disp</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">covmean</span> <span class="o">=</span> <span class="n">covmean</span><span class="p">.</span><span class="n">real</span>

    <span class="n">fid</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">mu1</span> <span class="o">-</span> <span class="n">mu2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="n">sigma1</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">covmean</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fid</span>

<span class="k">def</span> <span class="nf">get_features</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sh">"</span><span class="s">Extracting Features</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">feats</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">batch</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="n">features</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
   
<span class="c1"># device settings
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">VideoFeatureExtractor</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Number of videos, channels, frames, height, width
</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span>

<span class="c1"># Generate "real" videos using random integers in the range [0, 255]
</span><span class="n">real_videos</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">uint8</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>

<span class="c1"># Generate "generated" videos: add a small bias to simulate differences
# For instance, add a bias of 10 to the pixel values and clamp them to [0, 255]
</span><span class="n">gen_videos</span> <span class="o">=</span> <span class="n">real_videos</span> <span class="o">+</span> <span class="mf">10.0</span>
<span class="n">gen_videos</span> <span class="o">=</span> <span class="n">gen_videos</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>

<span class="c1"># Create DataLoaders
</span><span class="n">real_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">real_videos</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">gen_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">gen_videos</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># Extract features
</span><span class="n">real_feats</span> <span class="o">=</span> <span class="nf">get_features</span><span class="p">(</span><span class="n">real_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">gen_feats</span> <span class="o">=</span> <span class="nf">get_features</span><span class="p">(</span><span class="n">gen_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="c1"># Calculate FID
</span><span class="n">fid_score</span> <span class="o">=</span> <span class="nf">calculate_fid</span><span class="p">(</span><span class="n">real_feats</span><span class="p">,</span> <span class="n">gen_feats</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">FID: </span><span class="si">{</span><span class="n">fid_score</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Key Notes</strong>:</p> <ul> <li><strong>I3D Preprocessing</strong>: Videos must be normalized to <code class="language-plaintext highlighter-rouge">[0, 1]</code> and resized to <code class="language-plaintext highlighter-rouge">224x224</code>.</li> <li><strong>Hardware</strong>: Use GPUs for faster feature extraction.</li> </ul> <h3 id="2-psnr-and-ssim-for-videos"><strong>2. PSNR and SSIM for videos</strong></h3> <h3 id="mathematical-foundations"><strong>Mathematical Foundations</strong></h3> <ul> <li><strong>PSNR</strong>: Measures pixel-level accuracy</li> </ul> \[\text{PSNR} = 20 \cdot \log_{10}\left(\frac{\text{MAX}_I}{\sqrt{\text{MSE}}}\right)\] <p>with $\text{MAX}_I = 255$ for 8-bit images</p> <ul> <li> <p><strong>SSIM</strong>: Assesses perceptual quality through luminance $l$, contrast $c$, and structure $s$:</p> \[\text{SSIM}(x, y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}\] <p>$C_1, C_2$ are constants.</p> <p>We will not go through the details of the SSIM (Structural SIMilarity) formula, you can check them out here (with the source code of the method in <a href="https://scikit-image.org">skimage</a>):</p> <p>https://github.com/scikit-image/scikit-image/blob/main/skimage/metrics/_structural_similarity.py</p> </li> </ul> <h3 id="vectorized-implementation">Vectorized implementation:</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">skimage.metrics</span> <span class="kn">import</span> <span class="n">structural_similarity</span>

<span class="k">def</span> <span class="nf">calculate_psnr</span><span class="p">(</span><span class="n">video1</span><span class="p">,</span> <span class="n">video2</span><span class="p">):</span>
    <span class="c1"># Inputs: (N, C, T, H, W) in [0, 255]
</span>    <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">video1</span> <span class="o">-</span> <span class="n">video2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">psnr</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log10</span><span class="p">(</span><span class="mf">255.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">psnr</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_ssim</span><span class="p">(</span><span class="n">video1</span><span class="p">,</span> <span class="n">video2</span><span class="p">):</span>
    <span class="c1"># Inputs: (N, T, H, W, C) in [0, 255]
</span>    <span class="n">ssim_scores</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">vid1</span><span class="p">,</span> <span class="n">vid2</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">video1</span><span class="p">,</span> <span class="n">video2</span><span class="p">):</span>
        <span class="n">vid1</span> <span class="o">=</span> <span class="n">vid1</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">vid2</span> <span class="o">=</span> <span class="n">vid2</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">ssim_batch</span> <span class="o">=</span> <span class="p">[</span><span class="nf">structural_similarity</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">,</span> <span class="n">channel_axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_range</span><span class="o">=</span><span class="mi">255</span><span class="p">)</span> <span class="k">for</span> <span class="n">f1</span><span class="p">,</span> <span class="n">f2</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">vid1</span><span class="p">,</span> <span class="n">vid2</span><span class="p">)]</span>
        <span class="n">ssim_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ssim_batch</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">ssim_scores</span><span class="p">)</span>

<span class="c1"># Basic example Usage
</span><span class="n">real_video</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>  <span class="c1"># (N, T, H, W, C)
</span><span class="n">gen_video</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="n">psnr</span> <span class="o">=</span> <span class="nf">calculate_psnr</span><span class="p">(</span><span class="n">real_video</span><span class="p">,</span> <span class="n">gen_video</span><span class="p">)</span>
<span class="n">ssim</span> <span class="o">=</span> <span class="nf">calculate_ssim</span><span class="p">(</span><span class="n">real_video</span><span class="p">,</span> <span class="n">gen_video</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">PSNR: </span><span class="si">{</span><span class="n">psnr</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> dB, SSIM: </span><span class="si">{</span><span class="n">ssim</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="3-conclusion"><strong>3. Conclusion</strong></h3> <p>Quantitative metrics like FID, PSNR, and SSIM provide complementary insights into video quality. While FID evaluates high-level feature distributions, PSNR and SSIM focus on pixel and structural fidelity. For robust evaluation:</p> <ul> <li>Use <strong>FID</strong> with a pretrained 3D CNN for generative tasks.</li> <li>Combine <strong>PSNR</strong> and <strong>SSIM</strong> for compression/restoration tasks.</li> <li>Always pair quantitative metrics with <strong>human evaluation</strong>.</li> </ul> <h3 id="references"><strong>References</strong></h3> <ol> <li>FID: <a href="https://arxiv.org/abs/1706.08500">Heusel et al. (2017)</a></li> <li>SSIM: <a href="https://ieeexplore.ieee.org/document/1284395">Wang et al. (2004)</a></li> <li>I3D: <a href="https://arxiv.org/abs/1705.07750">Carreira &amp; Zisserman (2017)</a></li> </ol>]]></content><author><name></name></author><category term="FID"/><category term="PSNR"/><category term="SSIM"/><summary type="html"><![CDATA[These are some notes with some code around FID, SSIM, and PSNR metrics.]]></summary></entry></feed>